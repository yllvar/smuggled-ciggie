{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cellUniqueIdByVincent": "092ee"
      },
      "source": [
        "# Spatio-Temporal Surveillance and National Market Simulation\n",
        "# Illicit Cigarettes Study (ICS) Malaysia - Data Science Analysis\n",
        "\n",
        "## Project Overview\n",
        "This notebook implements a comprehensive data science analysis for:\n",
        "1. **Spatio-Temporal Surveillance**: Detection of illegal cigarette hotspots and forecasting\n",
        "2. **National Market Simulation**: Economic loss estimation and enforcement ROI analysis\n",
        "\n",
        "Based on the Illicit Cigarettes Study (ICS) Malaysia Jan 2024 Report\n",
        "\n",
        "## Key Objectives:\n",
        "- Extract and digitize data from the ICS PDF report\n",
        "- Implement Nielsen's weighting methodology \n",
        "- Detect spatial-temporal patterns and hotspots\n",
        "- Forecast illegal cigarette incidence by state\n",
        "- Simulate enforcement scenarios and ROI\n",
        "- Validate against authoritative report figures\n",
        "\n",
        "---\n",
        "\n",
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellUniqueIdByVincent": "1bab4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing required packages...\n",
            "Collecting geopandas\n",
            "  Downloading geopandas-1.1.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /opt/anaconda3/lib/python3.12/site-packages (from geopandas) (1.26.3)\n",
            "Collecting pyogrio>=0.7.2 (from geopandas)\n",
            "  Downloading pyogrio-0.12.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from geopandas) (24.2)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from geopandas) (2.1.2)\n",
            "Collecting pyproj>=3.5.0 (from geopandas)\n",
            "  Downloading pyproj-3.7.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (31 kB)\n",
            "Collecting shapely>=2.0.0 (from geopandas)\n",
            "  Downloading shapely-2.1.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=2.0.0->geopandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=2.0.0->geopandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=2.0.0->geopandas) (2023.3)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from pyogrio>=0.7.2->geopandas) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->geopandas) (1.16.0)\n",
            "Downloading geopandas-1.1.1-py3-none-any.whl (338 kB)\n",
            "Downloading pyogrio-0.12.1-cp312-cp312-macosx_12_0_arm64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pyproj-3.7.2-cp312-cp312-macosx_14_0_arm64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading shapely-2.1.2-cp312-cp312-macosx_11_0_arm64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: shapely, pyproj, pyogrio, geopandas\n",
            "Successfully installed geopandas-1.1.1 pyogrio-0.12.1 pyproj-3.7.2 shapely-2.1.2\n",
            "Successfully installed geopandas\n",
            "Collecting folium\n",
            "  Downloading folium-0.20.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting branca>=0.6.0 (from folium)\n",
            "  Downloading branca-0.8.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: jinja2>=2.9 in /opt/anaconda3/lib/python3.12/site-packages (from folium) (3.1.4)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from folium) (1.26.3)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from folium) (2.32.5)\n",
            "Requirement already satisfied: xyzservices in /opt/anaconda3/lib/python3.12/site-packages (from folium) (2022.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2>=2.9->folium) (2.1.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->folium) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->folium) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->folium) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->folium) (2025.4.26)\n",
            "Downloading folium-0.20.0-py2.py3-none-any.whl (113 kB)\n",
            "Downloading branca-0.8.2-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: branca, folium\n",
            "Successfully installed branca-0.8.2 folium-0.20.0\n",
            "Successfully installed folium\n",
            "Collecting prophet\n",
            "  Downloading prophet-1.2.1-py3-none-macosx_11_0_arm64.whl.metadata (3.5 kB)\n",
            "Collecting cmdstanpy>=1.0.4 (from prophet)\n",
            "  Downloading cmdstanpy-1.3.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /opt/anaconda3/lib/python3.12/site-packages (from prophet) (1.26.3)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from prophet) (3.10.5)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /opt/anaconda3/lib/python3.12/site-packages (from prophet) (2.1.2)\n",
            "Collecting holidays<1,>=0.25 (from prophet)\n",
            "  Downloading holidays-0.85-py3-none-any.whl.metadata (50 kB)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /opt/anaconda3/lib/python3.12/site-packages (from prophet) (4.67.1)\n",
            "Requirement already satisfied: importlib_resources in /opt/anaconda3/lib/python3.12/site-packages (from prophet) (6.5.2)\n",
            "Collecting stanio<2.0.0,>=0.4.0 (from cmdstanpy>=1.0.4->prophet)\n",
            "  Downloading stanio-0.5.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from holidays<1,>=0.25->prophet) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (3.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.0.4->prophet) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.0.4->prophet) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil->holidays<1,>=0.25->prophet) (1.16.0)\n",
            "Downloading prophet-1.2.1-py3-none-macosx_11_0_arm64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading cmdstanpy-1.3.0-py3-none-any.whl (99 kB)\n",
            "Downloading holidays-0.85-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stanio-0.5.1-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: stanio, holidays, cmdstanpy, prophet\n",
            "Successfully installed cmdstanpy-1.3.0 holidays-0.85 prophet-1.2.1 stanio-0.5.1\n",
            "Successfully installed prophet\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.8-py3-none-any.whl.metadata (43 kB)\n",
            "Collecting pdfminer.six==20251107 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20251107-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /opt/anaconda3/lib/python3.12/site-packages (from pdfplumber) (10.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-5.1.0-py3-none-macosx_11_0_arm64.whl.metadata (67 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pdfminer.six==20251107->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pdfminer.six==20251107->pdfplumber) (45.0.3)\n",
            "Requirement already satisfied: cffi>=1.14 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.21)\n",
            "Downloading pdfplumber-0.11.8-py3-none-any.whl (60 kB)\n",
            "Downloading pdfminer_six-20251107-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-5.1.0-py3-none-macosx_11_0_arm64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20251107 pdfplumber-0.11.8 pypdfium2-5.1.0\n",
            "Successfully installed pdfplumber\n",
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /opt/anaconda3/lib/python3.12/site-packages (from tabula-py) (2.1.2)\n",
            "Requirement already satisfied: numpy>1.24.4 in /opt/anaconda3/lib/python3.12/site-packages (from tabula-py) (1.26.3)\n",
            "Requirement already satisfied: distro in /opt/anaconda3/lib/python3.12/site-packages (from tabula-py) (1.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=0.25.3->tabula-py) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=0.25.3->tabula-py) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=0.25.3->tabula-py) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.16.0)\n",
            "Downloading tabula_py-2.10.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tabula-py\n",
            "Successfully installed tabula-py-2.10.0\n",
            "Successfully installed tabula-py\n",
            "Requirement already satisfied: plotly in /opt/anaconda3/lib/python3.12/site-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotly) (8.2.3)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from plotly) (24.2)\n",
            "Successfully installed plotly\n",
            "Requirement already satisfied: statsmodels in /opt/anaconda3/lib/python3.12/site-packages (0.14.2)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /opt/anaconda3/lib/python3.12/site-packages (from statsmodels) (1.26.3)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /opt/anaconda3/lib/python3.12/site-packages (from statsmodels) (1.13.1)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from statsmodels) (2.1.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/lib/python3.12/site-packages (from statsmodels) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2023.3)\n",
            "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
            "Successfully installed statsmodels\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
            "Successfully installed scikit-learn\n",
            "\n",
            "Installation complete!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"Successfully installed {package}\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"Failed to install {package}\")\n",
        "\n",
        "# Install spatial and specialized packages\n",
        "packages_to_install = [\n",
        "    'geopandas',\n",
        "    'folium',\n",
        "    'prophet',\n",
        "    'pdfplumber',\n",
        "    'tabula-py',\n",
        "    'plotly',\n",
        "    'statsmodels',\n",
        "    'scikit-learn'\n",
        "]\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "for package in packages_to_install:\n",
        "    install_package(package)\n",
        "\n",
        "print(\"\\nInstallation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellUniqueIdByVincent": "16ebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "Python environment ready for ICS Malaysia analysis\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Spatial analysis libraries\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point, Polygon\n",
        "import folium\n",
        "from folium import plugins\n",
        "\n",
        "# Time series and forecasting\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from prophet import Prophet\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# PDF processing\n",
        "import pdfplumber\n",
        "import tabula\n",
        "\n",
        "# Visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# File and directory management\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Python environment ready for ICS Malaysia analysis\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellUniqueIdByVincent": "042a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created directory: data/raw\n",
            "Created directory: data/processed\n",
            "Created directory: data/external\n",
            "Created directory: outputs/figures\n",
            "Created directory: outputs/reports\n",
            "Created directory: outputs/simulations\n",
            "\n",
            "✓ Found PDF file: Illicit-Cigarettes-Study--ICS--In-Malaysia--Jan-2024-Report.pdf\n",
            "File size: 1.72 MB\n",
            "\n",
            "Files in current directory:\n",
            "  - notebook-20251129_1607.ipynb\n",
            "  - Illicit-Cigarettes-Study--ICS--In-Malaysia--Jan-2024-Report.pdf\n",
            "\n",
            "Project structure created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Project Setup and Directory Structure\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Create project directory structure\n",
        "project_dirs = [\n",
        "    'data/raw',\n",
        "    'data/processed', \n",
        "    'data/external',\n",
        "    'outputs/figures',\n",
        "    'outputs/reports',\n",
        "    'outputs/simulations'\n",
        "]\n",
        "\n",
        "for dir_path in project_dirs:\n",
        "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Created directory: {dir_path}\")\n",
        "\n",
        "# Check if PDF file exists\n",
        "pdf_file = \"Illicit-Cigarettes-Study--ICS--In-Malaysia--Jan-2024-Report.pdf\"\n",
        "if os.path.exists(pdf_file):\n",
        "    print(f\"\\n✓ Found PDF file: {pdf_file}\")\n",
        "    file_size = os.path.getsize(pdf_file) / (1024*1024)  # Size in MB\n",
        "    print(f\"File size: {file_size:.2f} MB\")\n",
        "else:\n",
        "    print(f\"\\n✗ PDF file not found: {pdf_file}\")\n",
        "    print(\"Please ensure the PDF file is in the current directory\")\n",
        "\n",
        "# List all files in current directory\n",
        "print(\"\\nFiles in current directory:\")\n",
        "for file in os.listdir('.'):\n",
        "    if os.path.isfile(file):\n",
        "        print(f\"  - {file}\")\n",
        "\n",
        "print(\"\\nProject structure created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellUniqueIdByVincent": "ed9ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Examining PDF structure...\n",
            "PDF has 61 pages\n",
            "\n",
            "--- Page 1 ---\n",
            "Illicit Cigarettes Study (ICS)\n",
            "In Malaysia, Jan 2024 Report\n",
            "Report Commissioned by CMTM member companies\n",
            "© 2023 Nielsen Consumer LLC. All Rights Reserved. © 2023 Nielsen Consumer LLC. All Rights Reserved.\n",
            "\n",
            "--- Page 2 ---\n",
            "Table of contents\n",
            "3 Appendix: Nielsen’s ICS Research Objective & 29-60\n",
            "Illegal Cigarettes Criteria for Verification\n",
            "Methodology\n",
            "4\n",
            "Illegal Cigarettes Incidence at National Level\n",
            "5\n",
            "Illegal Cigarettes Incidence at Peninsular Malaysia Level\n",
            "6\n",
            "Illegal Cigarettes Incidence at East Malaysia Level\n",
            "7\n",
            "Top 10 Illegal Cigarettes Brands National Level\n",
            "8\n",
            "Tax Stamp Breakdown of Illegal Cigarettes\n",
            "Fake Tax Stamp Breakdown by Manufacturers / 9-12\n",
            "Importers\n",
            "13-14\n",
            "Illegal Cigarettes Incidence by States\n",
            "15-28\n",
            "Top 1...\n",
            "\n",
            "--- Page 3 ---\n",
            "Illegal Cigarettes Criteria For Verification\n",
            "Packs with at least one (1) of the following features:\n",
            "1. Absence of Registered Importers\n",
            "2. Unregistered Importers*\n",
            "3. Absence of Registered Manufacturers\n",
            "4. Unregistered Manufacturers*\n",
            "5. Packs with counterfeit (fake) Malaysian tax stamp**\n",
            "6. Packs without Malaysian tax stamp\n",
            "7. Packs with non-Malaysian tax stamp\n",
            "8. Unregistered Brand*\n",
            "9. Non Compliance to Ministry of Health’s Control of Tobacco Product Regulations (CTPR) 2004\n",
            "I. Absence or Non Comp...\n",
            "\n",
            "--- Page 4 ---\n",
            "Illegal Cigarettes Incidence At National Level :\n",
            "Overall increase by 1.1 ppt. vs. Nov, 2023\n",
            "Incidence (%)\n",
            "80 63.8 70\n",
            "62.3\n",
            "70 57.3 56.6 55.6 55.4 55.3 56.4\n",
            "60\n",
            "60 7.6 7.3\n",
            "50\n",
            "Products with Fake\n",
            "7.7 7.9\n",
            "Tax Stamp 50 8.7 10.7 10.7 10.6\n",
            "40\n",
            "40\n",
            "Smuggled Whites 42.7\n",
            "43.5 30\n",
            "30 42.3 41.4 39.5 38.0 36.8 38.5\n",
            "20\n",
            "20\n",
            "Smuggled Kreteks\n",
            "10\n",
            "10\n",
            "13.8\n",
            "11.2\n",
            "7.3 7.3 7.4 6.7 7.8 7.3\n",
            "0 0\n",
            "2019 2020 2021 2022 2023 Sep, 2023 Nov, 2023 Jan, 2024\n",
            "Volume (Billions sticks)\n",
            "Legal Cigarettes 7.3 6.7 6.9 7.1 7.1 0.5 0.6 0.5\n",
            "Illeg...\n",
            "\n",
            "--- Page 5 ---\n",
            "Illegal Cigarettes Incidence In Pen. Malaysia :\n",
            "Overall increase by 1.0 ppt. vs. Nov, 2023\n",
            "Incidence (%)\n",
            "80 70\n",
            "61.9\n",
            "59.9\n",
            "70\n",
            "60\n",
            "52.5 52.1 51.5 51.3 51.4 52.4\n",
            "60\n",
            "7.9\n",
            "Products with Fake 8.5 50\n",
            "Tax Stamp\n",
            "50\n",
            "9.2 9.4\n",
            "10.3 12.5\n",
            "12.5 12.5 40\n",
            "40\n",
            "Smuggled Whites 39.6 30\n",
            "39.9\n",
            "30\n",
            "35.9 35.5 33.7 31.6 30.8 32.2 20\n",
            "20\n",
            "Smuggled Kreteks\n",
            "10\n",
            "10\n",
            "14.4\n",
            "11.5\n",
            "7.4 7.2 7.5 7.2 8.1 7.7\n",
            "0 0\n",
            "2019 2020 2021 2022 2023 Sep, 2023 Nov, 2023 Jan, 2024\n",
            "Volume (Billions sticks)\n",
            "Legal Cigarettes 7.0 6.5 6.4 6.6 6.6 0.52 0.55 0.48\n",
            "Il...\n"
          ]
        }
      ],
      "source": [
        "# PDF Data Extraction and Analysis\n",
        "# Step 1: Examine PDF structure and extract key tables\n",
        "\n",
        "def examine_pdf_structure(pdf_path):\n",
        "    \"\"\"Examine the PDF structure and extract basic information\"\"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        print(f\"PDF has {len(pdf.pages)} pages\")\n",
        "        \n",
        "        # Extract text from first few pages to understand structure\n",
        "        for i, page in enumerate(pdf.pages[:5]):\n",
        "            print(f\"\\n--- Page {i+1} ---\")\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                # Show first 500 characters\n",
        "                print(text[:500] + \"...\" if len(text) > 500 else text)\n",
        "            else:\n",
        "                print(\"No text found on this page\")\n",
        "                \n",
        "        return pdf\n",
        "\n",
        "# Examine the PDF structure\n",
        "pdf_file = \"Illicit-Cigarettes-Study--ICS--In-Malaysia--Jan-2024-Report.pdf\"\n",
        "print(\"Examining PDF structure...\")\n",
        "pdf_obj = examine_pdf_structure(pdf_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellUniqueIdByVincent": "c3f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting key data from PDF...\n",
            "Processing page 1...\n",
            "Processing page 2...\n",
            "  Found national level data on page 2\n",
            "  Found state-level data on page 2\n",
            "  Found tax stamp data on page 2\n",
            "Processing page 3...\n",
            "Processing page 4...\n",
            "Found 8 table(s) on page 4\n",
            "  Table 1: 3 rows x 1 columns\n",
            "  Table 2: 3 rows x 1 columns\n",
            "  Table 3: 3 rows x 1 columns\n",
            "  Table 4: 3 rows x 1 columns\n",
            "  Table 5: 3 rows x 1 columns\n",
            "  Table 6: 3 rows x 1 columns\n",
            "  Table 7: 3 rows x 1 columns\n",
            "  Table 8: 3 rows x 1 columns\n",
            "  Found national level data on page 4\n",
            "  Found tax stamp data on page 4\n",
            "Processing page 5...\n",
            "Found 8 table(s) on page 5\n",
            "  Table 1: 3 rows x 1 columns\n",
            "  Table 2: 3 rows x 1 columns\n",
            "  Table 3: 3 rows x 1 columns\n",
            "  Table 4: 3 rows x 1 columns\n",
            "  Table 5: 3 rows x 1 columns\n",
            "  Table 6: 3 rows x 1 columns\n",
            "  Table 7: 3 rows x 1 columns\n",
            "  Table 8: 3 rows x 1 columns\n",
            "  Found tax stamp data on page 5\n",
            "Processing page 6...\n",
            "Found 6 table(s) on page 6\n",
            "  Table 1: 2 rows x 1 columns\n",
            "  Table 2: 2 rows x 1 columns\n",
            "  Table 3: 2 rows x 1 columns\n",
            "  Table 4: 2 rows x 1 columns\n",
            "  Table 5: 2 rows x 1 columns\n",
            "  Table 6: 2 rows x 1 columns\n",
            "  Found tax stamp data on page 6\n",
            "Processing page 7...\n",
            "Found 1 table(s) on page 7\n",
            "  Table 1: 17 rows x 7 columns\n",
            "Processing page 8...\n",
            "Found 11 table(s) on page 8\n",
            "  Table 1: 2 rows x 3 columns\n",
            "  Table 2: 2 rows x 1 columns\n",
            "  Table 3: 2 rows x 1 columns\n",
            "  Table 4: 2 rows x 1 columns\n",
            "  Table 5: 2 rows x 1 columns\n",
            "  Table 6: 2 rows x 1 columns\n",
            "  Table 7: 2 rows x 1 columns\n",
            "  Table 8: 2 rows x 1 columns\n",
            "  Table 9: 2 rows x 1 columns\n",
            "  Table 10: 2 rows x 1 columns\n",
            "  Table 11: 2 rows x 1 columns\n",
            "  Found tax stamp data on page 8\n",
            "Processing page 9...\n",
            "Found 1 table(s) on page 9\n",
            "  Table 1: 4 rows x 4 columns\n",
            "  Found tax stamp data on page 9\n",
            "Processing page 10...\n",
            "Found 1 table(s) on page 10\n",
            "  Table 1: 4 rows x 4 columns\n",
            "  Found tax stamp data on page 10\n",
            "Processing page 11...\n",
            "Found 1 table(s) on page 11\n",
            "  Table 1: 4 rows x 4 columns\n",
            "  Found tax stamp data on page 11\n",
            "Processing page 12...\n",
            "Found 12 table(s) on page 12\n",
            "  Table 2: 3 rows x 1 columns\n",
            "  Table 3: 2 rows x 1 columns\n",
            "  Table 4: 2 rows x 1 columns\n",
            "  Table 5: 2 rows x 1 columns\n",
            "  Table 6: 2 rows x 1 columns\n",
            "  Table 7: 2 rows x 1 columns\n",
            "  Table 8: 4 rows x 1 columns\n",
            "  Table 9: 2 rows x 1 columns\n",
            "  Table 10: 2 rows x 1 columns\n",
            "  Table 11: 2 rows x 1 columns\n",
            "  Table 12: 2 rows x 1 columns\n",
            "  Found tax stamp data on page 12\n",
            "Processing page 13...\n",
            "Found 4 table(s) on page 13\n",
            "  Table 1: 18 rows x 5 columns\n",
            "  Table 2: 18 rows x 4 columns\n",
            "  Table 3: 18 rows x 4 columns\n",
            "  Table 4: 17 rows x 1 columns\n",
            "Processing page 14...\n",
            "Found 4 table(s) on page 14\n",
            "  Table 1: 18 rows x 5 columns\n",
            "  Table 2: 18 rows x 4 columns\n",
            "  Table 3: 18 rows x 4 columns\n",
            "  Table 4: 17 rows x 1 columns\n",
            "Processing page 15...\n",
            "Found 1 table(s) on page 15\n",
            "  Table 1: 17 rows x 7 columns\n",
            "Processing page 16...\n",
            "Found 1 table(s) on page 16\n",
            "  Table 1: 17 rows x 7 columns\n",
            "Processing page 17...\n",
            "Found 1 table(s) on page 17\n",
            "  Table 1: 17 rows x 7 columns\n",
            "Processing page 18...\n",
            "Found 1 table(s) on page 18\n",
            "  Table 1: 17 rows x 7 columns\n",
            "Processing page 19...\n",
            "Found 1 table(s) on page 19\n",
            "  Table 1: 17 rows x 7 columns\n",
            "Processing page 20...\n",
            "Found 1 table(s) on page 20\n",
            "  Table 1: 17 rows x 7 columns\n",
            "Processing page 21...\n",
            "Found 1 table(s) on page 21\n",
            "  Table 1: 17 rows x 7 columns\n",
            "Processing page 22...\n",
            "Found 1 table(s) on page 22\n",
            "  Table 1: 17 rows x 7 columns\n",
            "Processing page 23...\n",
            "Found 1 table(s) on page 23\n",
            "  Table 1: 17 rows x 7 columns\n",
            "Processing page 24...\n",
            "Found 1 table(s) on page 24\n",
            "  Table 1: 14 rows x 7 columns\n",
            "Processing page 25...\n",
            "Found 1 table(s) on page 25\n",
            "  Table 1: 17 rows x 7 columns\n",
            "Processing page 26...\n",
            "Found 1 table(s) on page 26\n",
            "  Table 1: 17 rows x 7 columns\n",
            "Processing page 27...\n",
            "Found 1 table(s) on page 27\n",
            "  Table 1: 17 rows x 7 columns\n",
            "Processing page 28...\n",
            "Found 1 table(s) on page 28\n",
            "  Table 1: 17 rows x 7 columns\n",
            "Processing page 29...\n",
            "Processing page 30...\n",
            "  Found tax stamp data on page 30\n",
            "Processing page 31...\n",
            "Found 4 table(s) on page 31\n",
            "  Table 1: 3 rows x 1 columns\n",
            "  Table 2: 3 rows x 1 columns\n",
            "  Table 3: 2 rows x 1 columns\n",
            "  Table 4: 3 rows x 1 columns\n",
            "Processing page 32...\n",
            "Processing page 33...\n",
            "Found 1 table(s) on page 33\n",
            "  Table 1: 12 rows x 2 columns\n",
            "Processing page 34...\n",
            "Found 14 table(s) on page 34\n",
            "  Table 1: 2 rows x 1 columns\n",
            "  Table 2: 2 rows x 1 columns\n",
            "  Table 3: 2 rows x 1 columns\n",
            "  Table 4: 2 rows x 1 columns\n",
            "  Table 5: 2 rows x 1 columns\n",
            "  Table 6: 2 rows x 1 columns\n",
            "  Table 7: 2 rows x 1 columns\n",
            "  Table 8: 2 rows x 1 columns\n",
            "  Table 9: 2 rows x 1 columns\n",
            "  Table 10: 2 rows x 1 columns\n",
            "  Table 11: 2 rows x 1 columns\n",
            "  Table 12: 2 rows x 1 columns\n",
            "  Table 13: 2 rows x 1 columns\n",
            "  Table 14: 3 rows x 1 columns\n",
            "Processing page 35...\n",
            "Processing page 36...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cannot set gray non-stroke color because /'P251' is an invalid float value\n",
            "Cannot set gray non-stroke color because /'P253' is an invalid float value\n",
            "Cannot set gray non-stroke color because /'P261' is an invalid float value\n",
            "Cannot set gray non-stroke color because /'P263' is an invalid float value\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1 table(s) on page 36\n",
            "  Table 1: 29 rows x 4 columns\n",
            "Processing page 37...\n",
            "Processing page 38...\n",
            "Processing page 39...\n",
            "Processing page 40...\n",
            "Processing page 41...\n",
            "Processing page 42...\n",
            "Processing page 43...\n",
            "Processing page 44...\n",
            "Processing page 45...\n",
            "Processing page 46...\n",
            "  Found tax stamp data on page 46\n",
            "Processing page 47...\n",
            "  Found tax stamp data on page 47\n",
            "Processing page 48...\n",
            "  Found tax stamp data on page 48\n",
            "Processing page 49...\n",
            "Processing page 50...\n",
            "Processing page 51...\n",
            "Processing page 52...\n",
            "Processing page 53...\n",
            "Processing page 54...\n",
            "Found 1 table(s) on page 54\n",
            "  Table 1: 5 rows x 2 columns\n",
            "Processing page 55...\n",
            "Found 2 table(s) on page 55\n",
            "  Table 1: 16 rows x 3 columns\n",
            "  Table 2: 3 rows x 4 columns\n",
            "Processing page 56...\n",
            "Found 1 table(s) on page 56\n",
            "  Table 1: 16 rows x 4 columns\n",
            "Processing page 57...\n",
            "Found 1 table(s) on page 57\n",
            "  Table 1: 3 rows x 9 columns\n",
            "Processing page 58...\n",
            "Found 1 table(s) on page 58\n",
            "  Table 1: 16 rows x 9 columns\n",
            "Processing page 59...\n",
            "Found 1 table(s) on page 59\n",
            "  Table 1: 3 rows x 5 columns\n",
            "Processing page 60...\n",
            "Found 1 table(s) on page 60\n",
            "  Table 1: 16 rows x 5 columns\n",
            "Processing page 61...\n",
            "\n",
            "Extracted CSV files:\n",
            "  - page_60_table_1.csv\n",
            "  - page_10_table_1.csv\n",
            "  - page_4_table_6.csv\n",
            "  - page_12_table_8.csv\n",
            "  - page_22_table_1.csv\n",
            "  - page_12_table_9.csv\n",
            "  - page_4_table_7.csv\n",
            "  - page_8_table_1.csv\n",
            "  - page_8_table_3.csv\n",
            "  - page_19_table_1.csv\n",
            "  - page_4_table_5.csv\n",
            "  - page_4_table_4.csv\n",
            "  - page_8_table_2.csv\n",
            "  - page_8_table_6.csv\n",
            "  - page_4_table_1.csv\n",
            "  - page_8_table_7.csv\n",
            "  - page_8_table_5.csv\n",
            "  - page_15_table_1.csv\n",
            "  - page_4_table_3.csv\n",
            "  - page_4_table_2.csv\n",
            "  - page_57_table_1.csv\n",
            "  - page_27_table_1.csv\n",
            "  - page_8_table_4.csv\n",
            "  - page_31_table_3.csv\n",
            "  - page_54_table_1.csv\n",
            "  - page_24_table_1.csv\n",
            "  - page_8_table_11.csv\n",
            "  - page_34_table_14.csv\n",
            "  - page_34_table_6.csv\n",
            "  - page_34_table_7.csv\n",
            "  - page_16_table_1.csv\n",
            "  - page_8_table_10.csv\n",
            "  - page_13_table_4.csv\n",
            "  - page_31_table_2.csv\n",
            "  - page_5_table_8.csv\n",
            "  - page_7_table_1.csv\n",
            "  - page_34_table_5.csv\n",
            "  - page_34_table_4.csv\n",
            "  - page_31_table_1.csv\n",
            "  - page_13_table_3.csv\n",
            "  - page_58_table_1.csv\n",
            "  - page_28_table_1.csv\n",
            "  - page_34_table_12.csv\n",
            "  - page_34_table_13.csv\n",
            "  - page_34_table_1.csv\n",
            "  - page_13_table_2.csv\n",
            "  - page_31_table_4.csv\n",
            "  - page_21_table_1.csv\n",
            "  - page_34_table_3.csv\n",
            "  - page_34_table_11.csv\n",
            "  - page_34_table_10.csv\n",
            "  - page_34_table_2.csv\n",
            "  - page_13_table_1.csv\n",
            "  - page_5_table_2.csv\n",
            "  - page_12_table_11.csv\n",
            "  - page_26_table_1.csv\n",
            "  - page_56_table_1.csv\n",
            "  - page_12_table_10.csv\n",
            "  - page_5_table_3.csv\n",
            "  - page_14_table_1.csv\n",
            "  - page_14_table_3.csv\n",
            "  - page_5_table_1.csv\n",
            "  - page_12_table_12.csv\n",
            "  - page_33_table_1.csv\n",
            "  - page_14_table_2.csv\n",
            "  - page_5_table_4.csv\n",
            "  - page_34_table_9.csv\n",
            "  - page_34_table_8.csv\n",
            "  - page_18_table_1.csv\n",
            "  - page_36_table_1.csv\n",
            "  - page_5_table_5.csv\n",
            "  - page_5_table_7.csv\n",
            "  - page_23_table_1.csv\n",
            "  - page_9_table_1.csv\n",
            "  - page_11_table_1.csv\n",
            "  - page_14_table_4.csv\n",
            "  - page_5_table_6.csv\n",
            "  - page_8_table_9.csv\n",
            "  - page_6_table_6.csv\n",
            "  - page_8_table_8.csv\n",
            "  - page_20_table_1.csv\n",
            "  - page_6_table_5.csv\n",
            "  - page_12_table_2.csv\n",
            "  - page_12_table_3.csv\n",
            "  - page_6_table_4.csv\n",
            "  - page_59_table_1.csv\n",
            "  - page_12_table_7.csv\n",
            "  - page_55_table_2.csv\n",
            "  - page_12_table_6.csv\n",
            "  - page_4_table_8.csv\n",
            "  - page_6_table_1.csv\n",
            "  - page_17_table_1.csv\n",
            "  - page_6_table_3.csv\n",
            "  - page_12_table_4.csv\n",
            "  - page_25_table_1.csv\n",
            "  - page_12_table_5.csv\n",
            "  - page_55_table_1.csv\n",
            "  - page_6_table_2.csv\n",
            "\n",
            "Total files extracted: 98\n"
          ]
        }
      ],
      "source": [
        "# Extract key data tables from PDF\n",
        "def extract_key_data_from_pdf(pdf_path):\n",
        "    \"\"\"Extract key tables and data points from the ICS PDF\"\"\"\n",
        "    \n",
        "    extracted_data = {\n",
        "        'national_trends': [],\n",
        "        'state_data': [],\n",
        "        'brand_data': [],\n",
        "        'tax_stamp_data': [],\n",
        "        'volume_data': []\n",
        "    }\n",
        "    \n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        \n",
        "        # Look for tables in each page\n",
        "        for page_num, page in enumerate(pdf.pages):\n",
        "            print(f\"Processing page {page_num + 1}...\")\n",
        "            \n",
        "            # Extract tables from this page\n",
        "            tables = page.extract_tables()\n",
        "            \n",
        "            if tables:\n",
        "                print(f\"Found {len(tables)} table(s) on page {page_num + 1}\")\n",
        "                for i, table in enumerate(tables):\n",
        "                    if table and len(table) > 1:  # Valid table with header\n",
        "                        print(f\"  Table {i+1}: {len(table)} rows x {len(table[0]) if table[0] else 0} columns\")\n",
        "                        \n",
        "                        # Save table to CSV for inspection\n",
        "                        df = pd.DataFrame(table[1:], columns=table[0])\n",
        "                        df.to_csv(f'data/raw/page_{page_num+1}_table_{i+1}.csv', index=False)\n",
        "            \n",
        "            # Extract text for pattern matching\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                # Look for key patterns\n",
        "                if \"National Level\" in text and \"Incidence\" in text:\n",
        "                    print(f\"  Found national level data on page {page_num + 1}\")\n",
        "                \n",
        "                if \"by States\" in text:\n",
        "                    print(f\"  Found state-level data on page {page_num + 1}\")\n",
        "                \n",
        "                if \"Tax Stamp\" in text:\n",
        "                    print(f\"  Found tax stamp data on page {page_num + 1}\")\n",
        "    \n",
        "    return extracted_data\n",
        "\n",
        "# Extract data from PDF\n",
        "print(\"Extracting key data from PDF...\")\n",
        "data = extract_key_data_from_pdf(pdf_file)\n",
        "\n",
        "# List extracted CSV files\n",
        "print(\"\\nExtracted CSV files:\")\n",
        "raw_files = [f for f in os.listdir('data/raw') if f.endswith('.csv')]\n",
        "for file in raw_files:\n",
        "    print(f\"  - {file}\")\n",
        "    \n",
        "print(f\"\\nTotal files extracted: {len(raw_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellUniqueIdByVincent": "be097"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 largest tables by data points:\n",
            "============================================================\n",
            " 1. data/raw/page_58_table_1.csv    15 rows x  9 cols =  135 data points\n",
            " 2. data/raw/page_22_table_1.csv    16 rows x  7 cols =  112 data points\n",
            " 3. data/raw/page_19_table_1.csv    16 rows x  7 cols =  112 data points\n",
            " 4. data/raw/page_15_table_1.csv    16 rows x  7 cols =  112 data points\n",
            " 5. data/raw/page_27_table_1.csv    16 rows x  7 cols =  112 data points\n",
            " 6. data/raw/page_16_table_1.csv    16 rows x  7 cols =  112 data points\n",
            " 7. data/raw/page_7_table_1.csv     16 rows x  7 cols =  112 data points\n",
            " 8. data/raw/page_28_table_1.csv    16 rows x  7 cols =  112 data points\n",
            " 9. data/raw/page_21_table_1.csv    16 rows x  7 cols =  112 data points\n",
            "10. data/raw/page_26_table_1.csv    16 rows x  7 cols =  112 data points\n",
            "\n",
            "============================================================\n",
            "Examining top 3 largest tables:\n",
            "============================================================\n",
            "\n",
            "--- data/raw/page_58_table_1.csv ---\n",
            "Shape: (15, 9)\n",
            "Columns: ['State', 'Total packs\\ncollected\\n(Jan’24)', 'Number of\\nlegal packs\\ncollected', '(Unweighted)\\nVolume of legal\\npacks collected', 'Number of\\nillegal packs\\ncollected', '(Unweighted)\\nVolume of\\nillegal packs\\ncollected', 'Total\\nvolume\\n(Legal +\\nIllegal)', 'Incidence\\nof legal\\ncigarettes', 'Incidence\\nof illegal\\ncigarettes']\n",
            "\n",
            "First few rows:\n",
            "    State Total packs\\ncollected\\n(Jan’24) Number of\\nlegal packs\\ncollected  \\\n",
            "0     NaN                              NaN                               NaN   \n",
            "1  PERLIS                            1,200                               680   \n",
            "2   KEDAH                            1,200                               811   \n",
            "3  PENANG                            1,200                               615   \n",
            "4   PERAK                            1,200                               798   \n",
            "\n",
            "  (Unweighted)\\nVolume of legal\\npacks collected  \\\n",
            "0                                              A   \n",
            "1                                         13,600   \n",
            "2                                         16,220   \n",
            "3                                         12,300   \n",
            "4                                         15,960   \n",
            "\n",
            "  Number of\\nillegal packs\\ncollected  \\\n",
            "0                                 NaN   \n",
            "1                                 520   \n",
            "2                                 389   \n",
            "3                                 585   \n",
            "4                                 402   \n",
            "\n",
            "  (Unweighted)\\nVolume of\\nillegal packs\\ncollected  \\\n",
            "0                                                 B   \n",
            "1                                             9,752   \n",
            "2                                             7,771   \n",
            "3                                            11,274   \n",
            "4                                             7,948   \n",
            "\n",
            "  Total\\nvolume\\n(Legal +\\nIllegal) Incidence\\nof legal\\ncigarettes  \\\n",
            "0                         C = A + B                       D = A / C   \n",
            "1                            23,352                           58.2%   \n",
            "2                            23,991                           67.6%   \n",
            "3                            23,574                           52.2%   \n",
            "4                            23,908                           66.8%   \n",
            "\n",
            "  Incidence\\nof illegal\\ncigarettes  \n",
            "0                         E = B / C  \n",
            "1                             41.8%  \n",
            "2                             32.4%  \n",
            "3                             47.8%  \n",
            "4                             33.2%  \n",
            "Found Malaysian states: ['johor', 'kedah', 'kelantan', 'melaka', 'pahang', 'perak', 'perlis', 'sabah', 'sarawak', 'selangor']\n",
            "----------------------------------------\n",
            "\n",
            "--- data/raw/page_22_table_1.csv ---\n",
            "Shape: (16, 7)\n",
            "Columns: ['Unnamed: 0', 'Unnamed: 1', 'Jan, 2024', 'Unnamed: 3', 'Nov, 2023', 'Unnamed: 5', '2023']\n",
            "\n",
            "First few rows:\n",
            "  Unnamed: 0     Unnamed: 1 Jan, 2024  Unnamed: 3 Nov, 2023  Unnamed: 5  2023\n",
            "0        NaN            NaN       SOM         NaN       SOM         NaN   SOM\n",
            "1        NaN  Illegal Brand       (%)         NaN       (%)         NaN   (%)\n",
            "2         1.           John      16.3         NaN       4.8         NaN  10.7\n",
            "3         2.       Bosston*      13.3         NaN       9.7         NaN   4.9\n",
            "4         3.         Misto*       7.4         NaN       4.5         NaN   4.1\n",
            "----------------------------------------\n",
            "\n",
            "--- data/raw/page_19_table_1.csv ---\n",
            "Shape: (16, 7)\n",
            "Columns: ['Unnamed: 0', 'Unnamed: 1', 'Jan, 2024', 'Unnamed: 3', 'Nov, 2023', 'Unnamed: 5', '2023']\n",
            "\n",
            "First few rows:\n",
            "  Unnamed: 0     Unnamed: 1 Jan, 2024  Unnamed: 3 Nov, 2023  Unnamed: 5  2023\n",
            "0        NaN            NaN       SOM         NaN       SOM         NaN   SOM\n",
            "1        NaN  Illegal Brand       (%)         NaN       (%)         NaN   (%)\n",
            "2         1.           John       8.1         NaN       9.6         NaN  10.9\n",
            "3         2.           L.A.       6.2         NaN       7.1         NaN   4.8\n",
            "4         3.             U2       5.7         NaN       5.2         NaN   7.0\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Examine key extracted tables to understand data structure\n",
        "import glob\n",
        "\n",
        "def examine_extracted_tables():\n",
        "    \"\"\"Examine the extracted CSV files to understand data structure\"\"\"\n",
        "    \n",
        "    csv_files = glob.glob('data/raw/*.csv')\n",
        "    \n",
        "    # Sort by file size to identify the most substantial tables\n",
        "    file_info = []\n",
        "    for file in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            file_info.append({\n",
        "                'file': file,\n",
        "                'rows': len(df),\n",
        "                'cols': len(df.columns),\n",
        "                'size': df.shape[0] * df.shape[1]\n",
        "            })\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    # Sort by size (rows * cols)\n",
        "    file_info.sort(key=lambda x: x['size'], reverse=True)\n",
        "    \n",
        "    print(\"Top 10 largest tables by data points:\")\n",
        "    print(\"=\" * 60)\n",
        "    for i, info in enumerate(file_info[:10]):\n",
        "        print(f\"{i+1:2d}. {info['file']:30s} {info['rows']:3d} rows x {info['cols']:2d} cols = {info['size']:4d} data points\")\n",
        "    \n",
        "    return file_info\n",
        "\n",
        "# Examine the tables\n",
        "table_info = examine_extracted_tables()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Examining top 3 largest tables:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Look at the top 3 largest tables\n",
        "for i in range(min(3, len(table_info))):\n",
        "    file_path = table_info[i]['file']\n",
        "    print(f\"\\n--- {file_path} ---\")\n",
        "    \n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Shape: {df.shape}\")\n",
        "        print(f\"Columns: {list(df.columns)}\")\n",
        "        print(\"\\nFirst few rows:\")\n",
        "        print(df.head())\n",
        "        \n",
        "        # Check for state names or geographic indicators\n",
        "        text_content = df.to_string().lower()\n",
        "        states = ['johor', 'kedah', 'kelantan', 'melaka', 'negeri sembilan', \n",
        "                 'pahang', 'perak', 'perlis', 'pulau pinang', 'sabah', \n",
        "                 'sarawak', 'selangor', 'terengganu', 'kuala lumpur', 'putrajaya']\n",
        "        \n",
        "        found_states = [state for state in states if state in text_content]\n",
        "        if found_states:\n",
        "            print(f\"Found Malaysian states: {found_states}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "    \n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Economic Market Simulation and Enforcement ROI Analysis\n",
        "# Step 2: Process extracted data and create economic simulations\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "\n",
        "# Load and process the key state-level data\n",
        "def load_and_clean_state_data():\n",
        "    \"\"\"Load and clean the state-level incidence data\"\"\"\n",
        "    \n",
        "    # Load the main state data table\n",
        "    state_df = pd.read_csv('data/raw/page_58_table_1.csv')\n",
        "    \n",
        "    # Clean column names\n",
        "    state_df.columns = state_df.columns.str.replace('\\n', ' ').str.strip()\n",
        "    \n",
        "    # Remove header rows and clean data\n",
        "    state_df = state_df.dropna(subset=['State'])\n",
        "    state_df = state_df[~state_df['State'].isin(['A', 'B', 'C', 'D', 'E'])]\n",
        "    \n",
        "    # Clean numeric columns\n",
        "    numeric_cols = ['Total packs collected (Jan\\'24)', 'Number of legal packs collected', \n",
        "                   'Number of illegal packs collected']\n",
        "    \n",
        "    for col in numeric_cols:\n",
        "        if col in state_df.columns:\n",
        "            state_df[col] = state_df[col].astype(str).str.replace(',', '').astype(float)\n",
        "    \n",
        "    # Clean percentage columns\n",
        "    percentage_cols = ['Incidence of legal cigarettes', 'Incidence of illegal cigarettes']\n",
        "    for col in percentage_cols:\n",
        "        if col in state_df.columns:\n",
        "            state_df[col] = state_df[col].astype(str).str.replace('%', '').astype(float)\n",
        "    \n",
        "    return state_df\n",
        "\n",
        "# Load brand data\n",
        "def load_brand_data():\n",
        "    \"\"\"Load and clean brand market share data\"\"\"\n",
        "    \n",
        "    brand_files = ['data/raw/page_22_table_1.csv', 'data/raw/page_19_table_1.csv']\n",
        "    brand_data = []\n",
        "    \n",
        "    for file in brand_files:\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            # Clean and extract brand information\n",
        "            df = df.dropna(subset=['Unnamed: 1'])\n",
        "            df = df[df['Unnamed: 1'] != 'Illegal Brand']\n",
        "            \n",
        "            # Extract relevant columns\n",
        "            if 'Jan, 2024' in df.columns:\n",
        "                brands_df = df[['Unnamed: 1', 'Jan, 2024']].copy()\n",
        "                brands_df.columns = ['Brand', 'Market_Share_Jan2024']\n",
        "                brands_df['Market_Share_Jan2024'] = pd.to_numeric(brands_df['Market_Share_Jan2024'], errors='coerce')\n",
        "                brand_data.append(brands_df)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file}: {e}\")\n",
        "    \n",
        "    if brand_data:\n",
        "        return pd.concat(brand_data, ignore_index=True).dropna()\n",
        "    return pd.DataFrame()\n",
        "\n",
        "print(\"Loading and cleaning data...\")\n",
        "state_data = load_and_clean_state_data()\n",
        "brand_data = load_brand_data()\n",
        "\n",
        "print(f\"State data shape: {state_data.shape}\")\n",
        "print(f\"Brand data shape: {brand_data.shape}\")\n",
        "\n",
        "# Economic Simulation Model\n",
        "class EconomicSimulationModel:\n",
        "    \"\"\"Economic simulation model for illicit cigarette market analysis\"\"\"\n",
        "    \n",
        "    def __init__(self, state_data, brand_data):\n",
        "        self.state_data = state_data\n",
        "        self.brand_data = brand_data\n",
        "        \n",
        "        # Economic parameters (based on Malaysian market context)\n",
        "        self.avg_price_per_pack_legal = 17.0  # RM per pack (average legal price)\n",
        "        self.avg_price_per_pack_illegal = 8.0  # RM per pack (average illegal price)\n",
        "        self.tax_revenue_per_pack_legal = 12.0  # RM tax per legal pack\n",
        "        self.enforcement_cost_per_operation = 50000  # RM per enforcement operation\n",
        "        self.seizure_rate_per_operation = 0.15  # 15% seizure rate per operation\n",
        "        \n",
        "        # Market size parameters\n",
        "        self.total_smokers_malaysia = 5_000_000  # Estimated total smokers\n",
        "        self.avg_consumption_per_smoker = 15  # sticks per day\n",
        "        \n",
        "    def calculate_market_size(self):\n",
        "        \"\"\"Calculate total market size and economic impact\"\"\"\n",
        "        \n",
        "        # Calculate total packs consumption\n",
        "        total_daily_sticks = self.total_smokers_malaysia * self.avg_consumption_per_smoker\n",
        "        total_annual_sticks = total_daily_sticks * 365\n",
        "        total_annual_packs = total_annual_sticks / 20  # 20 sticks per pack\n",
        "        \n",
        "        # Use state data to get illegal incidence\n",
        "        avg_illegal_incidence = self.state_data['Incidence of illegal cigarettes'].mean() / 100\n",
        "        \n",
        "        # Calculate market segments\n",
        "        legal_packs = total_annual_packs * (1 - avg_illegal_incidence)\n",
        "        illegal_packs = total_annual_packs * avg_illegal_incidence\n",
        "        \n",
        "        # Calculate economic values\n",
        "        legal_market_value = legal_packs * self.avg_price_per_pack_legal\n",
        "        illegal_market_value = illegal_packs * self.avg_price_per_pack_illegal\n",
        "        tax_revenue_loss = illegal_packs * self.tax_revenue_per_pack_legal\n",
        "        \n",
        "        market_analysis = {\n",
        "            'total_annual_packs': total_annual_packs,\n",
        "            'legal_packs': legal_packs,\n",
        "            'illegal_packs': illegal_packs,\n",
        "            'legal_market_value_rm': legal_market_value,\n",
        "            'illegal_market_value_rm': illegal_market_value,\n",
        "            'tax_revenue_loss_rm': tax_revenue_loss,\n",
        "            'avg_illegal_incidence_pct': avg_illegal_incidence * 100\n",
        "        }\n",
        "        \n",
        "        return market_analysis\n",
        "    \n",
        "    def simulate_enforcement_scenarios(self, scenarios):\n",
        "        \"\"\"Simulate different enforcement scenarios and ROI\"\"\"\n",
        "        \n",
        "        results = []\n",
        "        \n",
        "        for scenario in scenarios:\n",
        "            scenario_name = scenario['name']\n",
        "            enforcement_intensity = scenario['intensity']  # Number of operations\n",
        "            effectiveness = scenario['effectiveness']  # Multiplier for seizure rate\n",
        "            \n",
        "            # Calculate enforcement costs\n",
        "            total_enforcement_cost = enforcement_intensity * self.enforcement_cost_per_operation\n",
        "            \n",
        "            # Calculate seizures\n",
        "            effective_seizure_rate = self.seizure_rate_per_operation * effectiveness\n",
        "            total_seized_packs = 0\n",
        "            \n",
        "            for _, state in self.state_data.iterrows():\n",
        "                market_analysis = self.calculate_market_size()\n",
        "                state_illegal_packs = (market_analysis['total_annual_packs'] / len(self.state_data)) * \\\n",
        "                                    (state['Incidence of illegal cigarettes'] / 100)\n",
        "                state_seizures = state_illegal_packs * effective_seizure_rate * enforcement_intensity\n",
        "                total_seized_packs += state_seizures\n",
        "            \n",
        "            # Calculate economic impact\n",
        "            seized_value = total_seized_packs * self.avg_price_per_pack_illegal\n",
        "            recovered_tax = total_seized_packs * self.tax_revenue_per_pack_legal\n",
        "            \n",
        "            # Calculate ROI\n",
        "            total_benefits = seized_value + recovered_tax\n",
        "            roi = (total_benefits - total_enforcement_cost) / total_enforcement_cost * 100\n",
        "            \n",
        "            # Calculate market impact\n",
        "            market_analysis = self.calculate_market_size()\n",
        "            market_reduction_pct = (total_seized_packs / (market_analysis['total_annual_packs'] * \n",
        "                                 (self.state_data['Incidence of illegal cigarettes'].mean() / 100))) * 100\n",
        "            \n",
        "            results.append({\n",
        "                'scenario': scenario_name,\n",
        "                'enforcement_operations': enforcement_intensity,\n",
        "                'total_cost_rm': total_enforcement_cost,\n",
        "                'seized_packs': total_seized_packs,\n",
        "                'seized_value_rm': seized_value,\n",
        "                'recovered_tax_rm': recovered_tax,\n",
        "                'total_benefits_rm': total_benefits,\n",
        "                'roi_pct': roi,\n",
        "                'market_reduction_pct': market_reduction_pct\n",
        "            })\n",
        "        \n",
        "        return pd.DataFrame(results)\n",
        "    \n",
        "    def generate_state_level_projections(self):\n",
        "        \"\"\"Generate state-level economic impact projections\"\"\"\n",
        "        \n",
        "        state_projections = []\n",
        "        \n",
        "        for _, state in self.state_data.iterrows():\n",
        "            state_name = state['State']\n",
        "            illegal_incidence = state['Incidence of illegal cigarettes'] / 100\n",
        "            \n",
        "            # Calculate state market size (proportional to population)\n",
        "            market_analysis = self.calculate_market_size()\n",
        "            state_market_share = 1 / len(self.state_data)  # Simplified equal distribution\n",
        "            state_total_packs = market_analysis['total_annual_packs'] * state_market_share\n",
        "            state_illegal_packs = state_total_packs * illegal_incidence\n",
        "            \n",
        "            # Economic impact\n",
        "            state_tax_loss = state_illegal_packs * self.tax_revenue_per_pack_legal\n",
        "            state_illegal_market = state_illegal_packs * self.avg_price_per_pack_illegal\n",
        "            \n",
        "            # Enforcement recommendations\n",
        "            recommended_operations = max(1, int(illegal_incidence * 10))\n",
        "            projected_seizures = state_illegal_packs * self.seizure_rate_per_operation * recommended_operations\n",
        "            projected_benefits = projected_seizures * (self.avg_price_per_pack_illegal + self.tax_revenue_per_pack_legal)\n",
        "            \n",
        "            state_projections.append({\n",
        "                'state': state_name,\n",
        "                'illegal_incidence_pct': illegal_incidence * 100,\n",
        "                'estimated_illegal_packs_annual': state_illegal_packs,\n",
        "                'tax_revenue_loss_rm': state_tax_loss,\n",
        "                'illegal_market_value_rm': state_illegal_market,\n",
        "                'recommended_enforcement_ops': recommended_operations,\n",
        "                'projected_annual_seizures': projected_seizures,\n",
        "                'projected_benefits_rm': projected_benefits,\n",
        "                'priority_score': illegal_incidence * 100  # Simple priority scoring\n",
        "            })\n",
        "        \n",
        "        return pd.DataFrame(state_projections)\n",
        "\n",
        "# Initialize simulation model\n",
        "sim_model = EconomicSimulationModel(state_data, brand_data)\n",
        "\n",
        "# Calculate market size\n",
        "market_analysis = sim_model.calculate_market_size()\n",
        "print(\"\\n=== MARKET SIZE ANALYSIS ===\")\n",
        "for key, value in market_analysis.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{key}: {value:,.2f}\")\n",
        "    else:\n",
        "        print(f\"{key}: {value:,}\")\n",
        "\n",
        "# Define enforcement scenarios\n",
        "enforcement_scenarios = [\n",
        "    {\n",
        "        'name': 'Current_Level',\n",
        "        'intensity': 50,\n",
        "        'effectiveness': 1.0\n",
        "    },\n",
        "    {\n",
        "        'name': 'Increased_Enforcement',\n",
        "        'intensity': 100,\n",
        "        'effectiveness': 1.2\n",
        "    },\n",
        "    {\n",
        "        'name': 'High_Intensity',\n",
        "        'intensity': 200,\n",
        "        'effectiveness': 1.5\n",
        "    },\n",
        "    {\n",
        "        'name': 'Targeted_Operations',\n",
        "        'intensity': 75,\n",
        "        'effectiveness': 2.0\n",
        "    }\n",
        "]\n",
        "\n",
        "# Run enforcement simulations\n",
        "simulation_results = sim_model.simulate_enforcement_scenarios(enforcement_scenarios)\n",
        "print(\"\\n=== ENFORCEMENT SCENARIO ANALYSIS ===\")\n",
        "print(simulation_results.to_string(index=False))\n",
        "\n",
        "# Generate state-level projections\n",
        "state_projections = sim_model.generate_state_level_projections()\n",
        "print(\"\\n=== STATE-LEVEL PROJECTIONS (Top 10 by Priority) ===\")\n",
        "top_states = state_projections.nlargest(10, 'priority_score')\n",
        "print(top_states.to_string(index=False))\n",
        "\n",
        "# Save simulation results to outputs/simulations\n",
        "print(\"\\nSaving simulation results...\")\n",
        "\n",
        "# Save market analysis\n",
        "with open('outputs/simulations/market_size_analysis.json', 'w') as f:\n",
        "    json.dump(market_analysis, f, indent=2, default=str)\n",
        "\n",
        "# Save enforcement scenarios\n",
        "simulation_results.to_csv('outputs/simulations/enforcement_scenarios.csv', index=False)\n",
        "\n",
        "# Save state projections\n",
        "state_projections.to_csv('outputs/simulations/state_level_projections.csv', index=False)\n",
        "\n",
        "# Create summary report\n",
        "summary_report = {\n",
        "    'analysis_date': datetime.now().isoformat(),\n",
        "    'market_summary': market_analysis,\n",
        "    'top_5_states_by_illegal_incidence': top_states[['state', 'illegal_incidence_pct', 'tax_revenue_loss_rm']].head(5).to_dict('records'),\n",
        "    'best_enforcement_scenario': simulation_results.loc[simulation_results['roi_pct'].idxmax()].to_dict(),\n",
        "    'total_potential_tax_recovery': market_analysis['tax_revenue_loss_rm']\n",
        "}\n",
        "\n",
        "with open('outputs/simulations/simulation_summary.json', 'w') as f:\n",
        "    json.dump(summary_report, f, indent=2, default=str)\n",
        "\n",
        "print(\"Simulation results saved to outputs/simulations/\")\n",
        "print(\"Files created:\")\n",
        "print(\"- market_size_analysis.json\")\n",
        "print(\"- enforcement_scenarios.csv\") \n",
        "print(\"- state_level_projections.csv\")\n",
        "print(\"- simulation_summary.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization of Economic Simulation Results\n",
        "# Step 3: Create comprehensive visualizations for simulation outputs\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Load simulation results\n",
        "simulation_results = pd.read_csv('outputs/simulations/enforcement_scenarios.csv')\n",
        "state_projections = pd.read_csv('outputs/simulations/state_level_projections.csv')\n",
        "\n",
        "with open('outputs/simulations/market_size_analysis.json', 'r') as f:\n",
        "    market_analysis = json.load(f)\n",
        "\n",
        "# 1. Market Size Visualization\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Malaysia Illicit Cigarette Market Economic Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Market breakdown\n",
        "market_categories = ['Legal Market', 'Illegal Market']\n",
        "market_values = [market_analysis['legal_market_value_rm'], market_analysis['illegal_market_value_rm']]\n",
        "colors = ['#2E8B57', '#DC143C']\n",
        "\n",
        "ax1.pie(market_values, labels=market_categories, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "ax1.set_title('Market Value Distribution (RM Billions)')\n",
        "\n",
        "# Tax revenue loss\n",
        "ax2.bar(['Tax Revenue Loss'], [market_analysis['tax_revenue_loss_rm']/1e9], color='#FF6B6B')\n",
        "ax2.set_title('Annual Tax Revenue Loss')\n",
        "ax2.set_ylabel('RM (Billions)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Incidence rates\n",
        "incidence_data = [market_analysis['avg_illegal_incidence_pct'], 100-market_analysis['avg_illegal_incidence_pct']]\n",
        "ax3.pie(incidence_data, labels=['Illegal', 'Legal'], colors=['#DC143C', '#2E8B57'], autopct='%1.1f%%')\n",
        "ax3.set_title('Market Incidence Rates')\n",
        "\n",
        "# Volume comparison\n",
        "volumes = [market_analysis['legal_packs']/1e9, market_analysis['illegal_packs']/1e9]\n",
        "ax4.bar(['Legal Packs', 'Illegal Packs'], volumes, color=['#2E8B57', '#DC143C'])\n",
        "ax4.set_title('Annual Volume (Billions of Packs)')\n",
        "ax4.set_ylabel('Packs (Billions)')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/simulations/market_analysis_visualization.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 2. Enforcement Scenario ROI Analysis\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Create scatter plot with bubble size representing benefits\n",
        "bubble_size = simulation_results['total_benefits_rm'] / 1e6  # Scale for bubble size\n",
        "scatter = ax.scatter(simulation_results['enforcement_operations'], \n",
        "                    simulation_results['roi_pct'],\n",
        "                    s=bubble_size*10,  # Scale bubble size\n",
        "                    c=simulation_results['market_reduction_pct'],\n",
        "                    cmap='RdYlBu_r',\n",
        "                    alpha=0.7,\n",
        "                    edgecolors='black')\n",
        "\n",
        "# Add labels for each scenario\n",
        "for i, row in simulation_results.iterrows():\n",
        "    ax.annotate(row['scenario'], \n",
        "                (row['enforcement_operations'], row['roi_pct']),\n",
        "                xytext=(5, 5), textcoords='offset points',\n",
        "                fontsize=10, fontweight='bold')\n",
        "\n",
        "ax.set_xlabel('Number of Enforcement Operations')\n",
        "ax.set_ylabel('ROI (%)')\n",
        "ax.set_title('Enforcement Scenario Analysis: ROI vs Operations Intensity')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Add colorbar for market reduction\n",
        "cbar = plt.colorbar(scatter)\n",
        "cbar.set_label('Market Reduction (%)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/simulations/enforcement_roi_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 3. State-Level Risk Heatmap\n",
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "\n",
        "# Prepare data for heatmap\n",
        "top_15_states = state_projections.nlargest(15, 'priority_score')\n",
        "heatmap_data = top_15_states.set_index('state')[['illegal_incidence_pct', 'tax_revenue_loss_rm', 'priority_score']]\n",
        "\n",
        "# Normalize data for better visualization\n",
        "heatmap_normalized = heatmap_data.copy()\n",
        "heatmap_normalized['tax_revenue_loss_rm'] = heatmap_normalized['tax_revenue_loss_rm'] / heatmap_normalized['tax_revenue_loss_rm'].max()\n",
        "\n",
        "# Create heatmap\n",
        "sns.heatmap(heatmap_normalized.T, \n",
        "            annot=True, \n",
        "            fmt='.2f',\n",
        "            cmap='Reds',\n",
        "            ax=ax,\n",
        "            cbar_kws={'label': 'Normalized Value'})\n",
        "\n",
        "ax.set_title('State-Level Risk Assessment Heatmap (Top 15 States)', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('States')\n",
        "ax.set_ylabel('Risk Factors')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/simulations/state_risk_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 4. Interactive Plotly Dashboard\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=('Market Size Breakdown', 'Enforcement ROI Comparison', \n",
        "                   'State Priority Ranking', 'Tax Loss by State'),\n",
        "    specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}],\n",
        "           [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
        ")\n",
        "\n",
        "# Market size pie chart\n",
        "fig.add_trace(\n",
        "    go.Pie(labels=['Legal Market', 'Illegal Market'],\n",
        "           values=[market_analysis['legal_market_value_rm'], market_analysis['illegal_market_value_rm']],\n",
        "           name=\"Market Size\"),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Enforcement ROI bar chart\n",
        "fig.add_trace(\n",
        "    go.Bar(x=simulation_results['scenario'], \n",
        "           y=simulation_results['roi_pct'],\n",
        "           name=\"ROI (%)\",\n",
        "           marker_color='lightblue'),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# State priority ranking\n",
        "top_10_states = state_projections.nlargest(10, 'priority_score')\n",
        "fig.add_trace(\n",
        "    go.Bar(x=top_10_states['state'], \n",
        "           y=top_10_states['priority_score'],\n",
        "           name=\"Priority Score\",\n",
        "           marker_color='coral'),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Tax loss by state\n",
        "fig.add_trace(\n",
        "    go.Bar(x=top_10_states['state'], \n",
        "           y=top_10_states['tax_revenue_loss_rm']/1e6,\n",
        "           name=\"Tax Loss (RM Millions)\",\n",
        "           marker_color='lightgreen'),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "fig.update_layout(height=800, showlegend=False, \n",
        "                  title_text=\"Malaysia Illicit Cigarette Market - Economic Simulation Dashboard\")\n",
        "\n",
        "# Save interactive plot\n",
        "fig.write_html('outputs/simulations/interactive_dashboard.html')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# 5. Cost-Benefit Analysis Chart\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Prepare data for cost-benefit analysis\n",
        "cost_benefit_data = simulation_results[['scenario', 'total_cost_rm', 'total_benefits_rm']].melt(\n",
        "    id_vars='scenario', \n",
        "    var_name='type', \n",
        "    value_name='amount_rm'\n",
        ")\n",
        "\n",
        "# Convert to millions for better readability\n",
        "cost_benefit_data['amount_rm'] = cost_benefit_data['amount_rm'] / 1e6\n",
        "\n",
        "# Create grouped bar chart\n",
        "sns.barplot(data=cost_benefit_data, x='scenario', y='amount_rm', hue='type', ax=ax)\n",
        "ax.set_title('Cost-Benefit Analysis by Enforcement Scenario (RM Millions)', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Enforcement Scenario')\n",
        "ax.set_ylabel('Amount (RM Millions)')\n",
        "ax.legend(title='Type', labels=['Cost', 'Benefits'])\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/simulations/cost_benefit_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 6. Create Executive Summary Report\n",
        "executive_summary = f\"\"\"\n",
        "# MALAYSIA ILLICIT CIGARETTE MARKET - ECONOMIC SIMULATION REPORT\n",
        "\n",
        "## Executive Summary\n",
        "- **Total Market Size**: {market_analysis['total_annual_packs']:,.0f} packs annually\n",
        "- **Illegal Market Share**: {market_analysis['avg_illegal_incidence_pct']:.1f}%\n",
        "- **Annual Tax Revenue Loss**: RM {market_analysis['tax_revenue_loss_rm']:,.2f}\n",
        "- **Illegal Market Value**: RM {market_analysis['illegal_market_value_rm']:,.2f}\n",
        "\n",
        "## Key Findings\n",
        "\n",
        "### 1. Market Impact\n",
        "The illicit cigarette market represents a significant economic challenge with:\n",
        "- Tax revenue losses exceeding RM {market_analysis['tax_revenue_loss_rm']/1e9:.1f} billion annually\n",
        "- Illegal market valued at RM {market_analysis['illegal_market_value_rm']/1e9:.1f} billion\n",
        "- Average illegal incidence of {market_analysis['avg_illegal_incidence_pct']:.1f}% nationally\n",
        "\n",
        "### 2. Enforcement Effectiveness\n",
        "Best performing scenario: {simulation_results.loc[simulation_results['roi_pct'].idxmax(), 'scenario']}\n",
        "- ROI: {simulation_results['roi_pct'].max():.1f}%\n",
        "- Market reduction: {simulation_results.loc[simulation_results['roi_pct'].idxmax(), 'market_reduction_pct']:.1f}%\n",
        "- Cost-benefit ratio: {simulation_results.loc[simulation_results['roi_pct'].idxmax(), 'total_benefits_rm']/simulation_results.loc[simulation_results['roi_pct'].idxmax(), 'total_cost_rm']:.2f}\n",
        "\n",
        "### 3. State-Level Priorities\n",
        "Top 5 high-priority states for enforcement:\n",
        "\"\"\"\n",
        "\n",
        "for i, (_, state) in enumerate(top_states.head(5).iterrows()):\n",
        "    executive_summary += f\"\"\"\n",
        "{i+1}. **{state['state']}** - Incidence: {state['illegal_incidence_pct']:.1f}%, Tax Loss: RM {state['tax_revenue_loss_rm']/1e6:.1f}M\n",
        "\"\"\"\n",
        "\n",
        "executive_summary += f\"\"\"\n",
        "\n",
        "## Recommendations\n",
        "1. **Targeted Enforcement**: Focus on high-priority states with {top_states['priority_score'].mean():.1f}%+ illegal incidence\n",
        "2. **Resource Optimization**: Implement {simulation_results.loc[simulation_results['roi_pct'].idxmax(), 'scenario']} scenario for optimal ROI\n",
        "3. **Market Intelligence**: Strengthen monitoring in states with highest tax revenue losses\n",
        "4. **Policy Impact**: Current enforcement could recover up to RM {simulation_results['total_benefits_rm'].max()/1e6:.1f}M annually\n",
        "\n",
        "## Generated Files\n",
        "- Market analysis visualizations\n",
        "- Enforcement scenario comparisons  \n",
        "- State-level risk assessments\n",
        "- Interactive dashboard for ongoing monitoring\n",
        "\n",
        "Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\"\"\"\n",
        "\n",
        "# Save executive summary\n",
        "with open('outputs/simulations/executive_summary.md', 'w') as f:\n",
        "    f.write(executive_summary)\n",
        "\n",
        "print(\"✅ Economic simulation visualizations and reports generated successfully!\")\n",
        "print(\"\\nFiles created in outputs/simulations/:\")\n",
        "print(\"- market_analysis_visualization.png\")\n",
        "print(\"- enforcement_roi_analysis.png\") \n",
        "print(\"- state_risk_heatmap.png\")\n",
        "print(\"- interactive_dashboard.html\")\n",
        "print(\"- cost_benefit_analysis.png\")\n",
        "print(\"- executive_summary.md\")\n",
        "\n",
        "print(f\"\\n📊 Key Insights:\")\n",
        "print(f\"• Annual tax revenue loss: RM {market_analysis['tax_revenue_loss_rm']/1e9:.2f} billion\")\n",
        "print(f\"• Best enforcement scenario: {simulation_results.loc[simulation_results['roi_pct'].idxmax(), 'scenario']}\")\n",
        "print(f\"• Maximum ROI achievable: {simulation_results['roi_pct'].max():.1f}%\")\n",
        "print(f\"• High-priority states identified: {len(top_states[top_states['priority_score'] > 50])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time Series Forecasting and Predictive Analytics\n",
        "# Step 4: Implement forecasting models for future market trends\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from prophet import Prophet\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Create synthetic historical data based on extracted trends\n",
        "def create_historical_time_series():\n",
        "    \"\"\"Create historical time series data based on PDF trends\"\"\"\n",
        "    \n",
        "    # Date range from 2019 to 2024\n",
        "    dates = pd.date_range(start='2019-01-01', end='2024-01-31', freq='M')\n",
        "    \n",
        "    # Base trends from PDF data (extracted from visualizations)\n",
        "    # National level trends showing increase from ~55% to ~63%\n",
        "    base_incidence_2019 = 55.0\n",
        "    base_incidence_2024 = 63.8\n",
        "    \n",
        "    # Create trend with some seasonality and noise\n",
        "    trend_values = np.linspace(base_incidence_2019, base_incidence_2024, len(dates))\n",
        "    \n",
        "    # Add seasonal patterns (higher during certain periods)\n",
        "    seasonal_pattern = 2 * np.sin(2 * np.pi * np.arange(len(dates)) / 12)  # Annual seasonality\n",
        "    noise = np.random.normal(0, 1, len(dates))\n",
        "    \n",
        "    # Combine components\n",
        "    illegal_incidence = trend_values + seasonal_pattern + noise\n",
        "    illegal_incidence = np.clip(illegal_incidence, 20, 80)  # Keep within reasonable bounds\n",
        "    \n",
        "    # Create DataFrame\n",
        "    historical_data = pd.DataFrame({\n",
        "        'ds': dates,\n",
        "        'y': illegal_incidence\n",
        "    })\n",
        "    \n",
        "    return historical_data\n",
        "\n",
        "# Create state-level historical data\n",
        "def create_state_historical_data(state_data):\n",
        "    \"\"\"Create historical data for each state\"\"\"\n",
        "    \n",
        "    states_data = {}\n",
        "    \n",
        "    for _, state in state_data.iterrows():\n",
        "        state_name = state['State']\n",
        "        current_incidence = state['Incidence of illegal cigarettes']\n",
        "        \n",
        "        # Create historical trend for this state\n",
        "        dates = pd.date_range(start='2019-01-01', end='2024-01-31', freq='M')\n",
        "        \n",
        "        # State-specific trend (some states higher, some lower)\n",
        "        state_factor = current_incidence / 50.0  # Normalize around 50%\n",
        "        base_trend = 40 + (state_factor * 15)  # Base incidence varies by state\n",
        "        \n",
        "        trend_values = np.linspace(base_trend - 5, current_incidence, len(dates))\n",
        "        seasonal_pattern = 1.5 * np.sin(2 * np.pi * np.arange(len(dates)) / 12)\n",
        "        noise = np.random.normal(0, 0.8, len(dates))\n",
        "        \n",
        "        state_incidence = trend_values + seasonal_pattern + noise\n",
        "        state_incidence = np.clip(state_incidence, 10, 90)\n",
        "        \n",
        "        states_data[state_name] = pd.DataFrame({\n",
        "            'ds': dates,\n",
        "            'y': state_incidence\n",
        "        })\n",
        "    \n",
        "    return states_data\n",
        "\n",
        "print(\"Creating historical time series data...\")\n",
        "national_historical = create_historical_time_series()\n",
        "states_historical = create_state_historical_data(state_data)\n",
        "\n",
        "# Prophet Forecasting for National Level\n",
        "print(\"\\\\n=== NATIONAL LEVEL FORECASTING ===\")\n",
        "\n",
        "# Initialize and fit Prophet model\n",
        "prophet_model = Prophet(\n",
        "    yearly_seasonality=True,\n",
        "    monthly_seasonality=True,\n",
        "    daily_seasonality=False,\n",
        "    changepoint_prior_scale=0.05,\n",
        "    seasonality_prior_scale=10.0,\n",
        "    holidays_prior_scale=10.0\n",
        ")\n",
        "\n",
        "# Add custom seasonality for enforcement effects\n",
        "prophet_model.add_seasonality(name='enforcement_effect', period=365.25, fourier_order=3)\n",
        "\n",
        "# Fit the model\n",
        "prophet_model.fit(national_historical)\n",
        "\n",
        "# Create future dataframe for 24 months forecast\n",
        "future = prophet_model.make_future_dataframe(periods=24, freq='M')\n",
        "\n",
        "# Make predictions\n",
        "forecast = prophet_model.predict(future)\n",
        "\n",
        "# Extract forecast components\n",
        "fig = prophet_model.plot_components(forecast, figsize=(15, 10))\n",
        "plt.suptitle('National Illicit Cigarette Incidence - Forecast Components', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/simulations/national_forecast_components.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot forecast\n",
        "fig, ax = plt.subplots(figsize=(15, 8))\n",
        "ax.plot(national_historical['ds'], national_historical['y'], 'o-', label='Historical', color='blue', alpha=0.7)\n",
        "ax.plot(forecast['ds'], forecast['yhat'], '--', label='Forecast', color='red', linewidth=2)\n",
        "ax.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'], alpha=0.3, color='red', label='Uncertainty Interval')\n",
        "ax.set_title('National Illicit Cigarette Incidence Forecast (24 Months)', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Illegal Incidence (%)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/simulations/national_forecast_plot.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# State-level forecasting\n",
        "print(\"\\\\n=== STATE-LEVEL FORECASTING ===\")\n",
        "\n",
        "state_forecasts = {}\n",
        "forecast_summary = []\n",
        "\n",
        "for state_name, state_data in states_historical.items():\n",
        "    try:\n",
        "        # Initialize Prophet model for this state\n",
        "        state_model = Prophet(\n",
        "            yearly_seasonality=True,\n",
        "            monthly_seasonality=False,\n",
        "            daily_seasonality=False,\n",
        "            changepoint_prior_scale=0.1\n",
        "        )\n",
        "        \n",
        "        # Fit model\n",
        "        state_model.fit(state_data)\n",
        "        \n",
        "        # Create future dataframe\n",
        "        state_future = state_model.make_future_dataframe(periods=24, freq='M')\n",
        "        \n",
        "        # Make predictions\n",
        "        state_forecast = state_model.predict(state_future)\n",
        "        \n",
        "        # Store results\n",
        "        state_forecasts[state_name] = state_forecast\n",
        "        \n",
        "        # Extract key metrics\n",
        "        current_incidence = state_data['y'].iloc[-1]\n",
        "        forecast_12m = state_forecast[state_forecast['ds'] > state_data['ds'].iloc[-1]]['yhat'].iloc[11]\n",
        "        forecast_24m = state_forecast[state_forecast['ds'] > state_data['ds'].iloc[-1]]['yhat'].iloc[23]\n",
        "        \n",
        "        forecast_summary.append({\n",
        "            'state': state_name,\n",
        "            'current_incidence': current_incidence,\n",
        "            'forecast_12m': forecast_12m,\n",
        "            'forecast_24m': forecast_24m,\n",
        "            'trend_12m': ((forecast_12m - current_incidence) / current_incidence) * 100,\n",
        "            'trend_24m': ((forecast_24m - current_incidence) / current_incidence) * 100\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error forecasting for {state_name}: {e}\")\n",
        "\n",
        "# Create forecast summary DataFrame\n",
        "forecast_df = pd.DataFrame(forecast_summary)\n",
        "forecast_df = forecast_df.sort_values('forecast_24m', ascending=False)\n",
        "\n",
        "print(\"State Forecast Summary (Top 10 by 24-month forecast):\")\n",
        "print(forecast_df.head(10).to_string(index=False))\n",
        "\n",
        "# Visualization of state forecasts\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "# Current vs Forecast comparison\n",
        "top_10_states = forecast_df.head(10)\n",
        "x_pos = np.arange(len(top_10_states))\n",
        "\n",
        "ax1.bar(x_pos - 0.2, top_10_states['current_incidence'], 0.4, label='Current', color='lightblue')\n",
        "ax1.bar(x_pos + 0.2, top_10_states['forecast_24m'], 0.4, label='24M Forecast', color='coral')\n",
        "ax1.set_xlabel('States')\n",
        "ax1.set_ylabel('Illegal Incidence (%)')\n",
        "ax1.set_title('Current vs 24-Month Forecast (Top 10 States)')\n",
        "ax1.set_xticks(x_pos)\n",
        "ax1.set_xticklabels(top_10_states['state'], rotation=45, ha='right')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Trend analysis\n",
        "ax2.bar(top_10_states['state'], top_10_states['trend_24m'], color='green' if top_10_states['trend_24m'].mean() > 0 else 'red')\n",
        "ax2.set_xlabel('States')\n",
        "ax2.set_ylabel('Expected Change (%)')\n",
        "ax2.set_title('24-Month Trend Analysis')\n",
        "ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/simulations/state_forecast_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Risk Assessment based on forecasts\n",
        "def assess_forecast_risks(forecast_df):\n",
        "    \"\"\"Assess risks based on forecast trends\"\"\"\n",
        "    \n",
        "    risk_assessment = []\n",
        "    \n",
        "    for _, row in forecast_df.iterrows():\n",
        "        state = row['state']\n",
        "        current = row['current_incidence']\n",
        "        forecast_24m = row['forecast_24m']\n",
        "        trend_24m = row['trend_24m']\n",
        "        \n",
        "        # Risk scoring\n",
        "        risk_score = 0\n",
        "        \n",
        "        # High current incidence (>50%)\n",
        "        if current > 50:\n",
        "            risk_score += 3\n",
        "        elif current > 40:\n",
        "            risk_score += 2\n",
        "        elif current > 30:\n",
        "            risk_score += 1\n",
        "            \n",
        "        # Increasing trend\n",
        "        if trend_24m > 10:\n",
        "            risk_score += 3\n",
        "        elif trend_24m > 5:\n",
        "            risk_score += 2\n",
        "        elif trend_24m > 0:\n",
        "            risk_score += 1\n",
        "        elif trend_24m < -5:\n",
        "            risk_score -= 1\n",
        "            \n",
        "        # Risk categorization\n",
        "        if risk_score >= 5:\n",
        "            risk_level = 'Critical'\n",
        "        elif risk_score >= 3:\n",
        "            risk_level = 'High'\n",
        "        elif risk_score >= 1:\n",
        "            risk_level = 'Medium'\n",
        "        else:\n",
        "            risk_level = 'Low'\n",
        "            \n",
        "        risk_assessment.append({\n",
        "            'state': state,\n",
        "            'current_incidence': current,\n",
        "            'forecast_24m': forecast_24m,\n",
        "            'trend_24m': trend_24m,\n",
        "            'risk_score': risk_score,\n",
        "            'risk_level': risk_level\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(risk_assessment).sort_values('risk_score', ascending=False)\n",
        "\n",
        "# Generate risk assessment\n",
        "risk_assessment = assess_forecast_risks(forecast_df)\n",
        "\n",
        "print(\"\\\\n=== FORECAST-BASED RISK ASSESSMENT ===\")\n",
        "print(\"Top 10 High-Risk States:\")\n",
        "print(risk_assessment.head(10)[['state', 'current_incidence', 'forecast_24m', 'trend_24m', 'risk_level']].to_string(index=False))\n",
        "\n",
        "# Save forecasting results\n",
        "forecast_df.to_csv('outputs/simulations/state_forecasts.csv', index=False)\n",
        "risk_assessment.to_csv('outputs/simulations/risk_assessment.csv', index=False)\n",
        "\n",
        "# Save forecast data for interactive use\n",
        "forecast_data = {\n",
        "    'national_forecast': forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(24).to_dict('records'),\n",
        "    'state_forecasts': {k: v[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(24).to_dict('records') \n",
        "                       for k, v in state_forecasts.items()},\n",
        "    'risk_assessment': risk_assessment.to_dict('records'),\n",
        "    'generated_date': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('outputs/simulations/forecast_data.json', 'w') as f:\n",
        "    json.dump(forecast_data, f, indent=2, default=str)\n",
        "\n",
        "print(\"\\\\n✅ Forecasting analysis completed!\")\n",
        "print(\"Files created:\")\n",
        "print(\"- national_forecast_components.png\")\n",
        "print(\"- national_forecast_plot.png\")\n",
        "print(\"- state_forecast_comparison.png\")\n",
        "print(\"- state_forecasts.csv\")\n",
        "print(\"- risk_assessment.csv\")\n",
        "print(\"- forecast_data.json\")\n",
        "\n",
        "print(f\"\\\\n📈 Key Forecast Insights:\")\n",
        "print(f\"• National 24M forecast: {forecast['yhat'].iloc[-1]:.1f}%\")\n",
        "print(f\"• Highest risk state: {risk_assessment.iloc[0]['state']} ({risk_assessment.iloc[0]['risk_level']} risk)\")\n",
        "print(f\"• States with increasing trends: {len(risk_assessment[risk_assessment['trend_24m'] > 0])}\")\n",
        "print(f\"• Critical risk states: {len(risk_assessment[risk_assessment['risk_level'] == 'Critical'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellUniqueIdByVincent": "7bafe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Extraction Complete! Created 6 CSV files:\n",
            "\n",
            "1. national_state_market_volume.csv\n",
            "    state  legal_sticks_000  illegal_sticks_000  total_sticks_000  \\\n",
            "0  Perlis              2945                2112              5057   \n",
            "1   Kedah             20617                9878             30495   \n",
            "2  Penang             36144               33129             69273   \n",
            "\n",
            "   incidence_percent  \n",
            "0               41.8  \n",
            "1               32.4  \n",
            "2               47.8  \n",
            "\n",
            "Shape: (15, 5)\n",
            "\n",
            "2. fake_tax_stamp_importers.csv\n",
            "                        importer  jan_2024_incidence  nov_2023_incidence  \\\n",
            "0       Mahata Tobacco Marketing                 2.6                 2.1   \n",
            "1          JTM Tobacco Marketing                 2.0                 1.8   \n",
            "2  Neptune Tobacco International                 0.3                 0.0   \n",
            "\n",
            "   sep_2023_incidence  jul_2023_incidence  \n",
            "0                 1.5                 3.3  \n",
            "1                 1.4                 1.4  \n",
            "2                 0.0                 0.0  \n",
            "\n",
            "Shape: (10, 5)\n",
            "\n",
            "3. top_illegal_brands_by_state.csv\n",
            "    state         brand  market_share_percent  rank\n",
            "0  Perlis          John                  32.5     1\n",
            "1  Perlis  Gudang Garam                   4.2     2\n",
            "2  Perlis          Vess                   4.1     3\n",
            "3   Kedah          John                  14.3     1\n",
            "4   Kedah         Tiara                   6.0     2\n",
            "5   Kedah         Misto                   3.2     3\n",
            "\n",
            "Shape: (42, 4)\n",
            "\n",
            "4. state_incidence_trends.csv\n",
            "    state    period  illegal_incidence_percent\n",
            "0  Perlis      2023                       50.6\n",
            "1  Perlis  Sep_2023                       51.8\n",
            "2  Perlis  Nov_2023                       51.8\n",
            "3  Perlis  Jan_2024                       41.8\n",
            "\n",
            "Shape: (72, 3)\n",
            "\n",
            "5. illegal_packs_by_category.csv\n",
            "              category  jan_2024_incidence  nov_2023_incidence  2023_incidence\n",
            "0    Without Tax Stamp                45.8                44.7            46.9\n",
            "1  With Fake Tax Stamp                10.6                10.6             8.7\n",
            "2      Smuggled Whites                38.5                38.4            39.5\n",
            "3     Smuggled Kreteks                 7.3                 7.3             7.4\n",
            "\n",
            "Shape: (4, 4)\n",
            "\n",
            "6. pack_level_fields_reference.csv\n",
            "           field_category       field_name                     description  \\\n",
            "0  Collection Information            State  State where pack was collected   \n",
            "1  Collection Information         District           District within state   \n",
            "2  Collection Information  Collection Date              Date of collection   \n",
            "3  Product Identification            Brand                      Brand name   \n",
            "\n",
            "     data_type  \n",
            "0  Categorical  \n",
            "1  Categorical  \n",
            "2         Date  \n",
            "3  Categorical  \n",
            "\n",
            "Shape: (14, 4)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Create the dataframes based on the PDF content\n",
        "\n",
        "# 1. National & State Market Volume Table\n",
        "market_volume_data = {\n",
        "    'state': ['Perlis', 'Kedah', 'Penang', 'Perak', 'Selangor', 'WP KL', 'N.Sembilan', \n",
        "              'Melaka', 'Johor', 'Pahang', 'Terengganu', 'Kelantan', 'Sabah', 'Sarawak', 'National'],\n",
        "    'legal_sticks_000': [2945, 20617, 36144, 38248, 126852, 60117, 25263, \n",
        "                         18453, 99921, 22126, 10149, 14253, 18884, 16347, 510319],\n",
        "    'illegal_sticks_000': [2112, 9878, 33129, 19048, 102534, 43776, 18331, \n",
        "                           23713, 132262, 92479, 24272, 21622, 70680, 66596, 660432],\n",
        "    'total_sticks_000': [5057, 30495, 69273, 57296, 229386, 103893, 43594, \n",
        "                         42166, 232183, 114606, 34422, 35874, 89564, 82943, 1170751],\n",
        "    'incidence_percent': [41.8, 32.4, 47.8, 33.2, 44.7, 42.1, 42.0, \n",
        "                          56.2, 57.0, 80.7, 70.5, 60.3, 78.9, 80.3, 56.4]\n",
        "}\n",
        "\n",
        "market_volume_df = pd.DataFrame(market_volume_data)\n",
        "\n",
        "# 2. Fake Tax Stamp Incidence by Importer\n",
        "fake_stamp_data = {\n",
        "    'importer': ['Mahata Tobacco Marketing', 'JTM Tobacco Marketing', 'Neptune Tobacco International',\n",
        "                 'Milton Tobacco International', 'Gillion Tobacco', 'Global Tobacco Manufacturer',\n",
        "                 'Legasi Jutawan', 'Pacific Tobacco Company', 'VTI Marketing', 'Global Resource 55'],\n",
        "    'jan_2024_incidence': [2.6, 2.0, 0.3, 1.2, 0.4, 0.0, 0.0, 0.0, 0.4, 0.0],\n",
        "    'nov_2023_incidence': [2.1, 1.8, 0.0, 1.4, 0.2, 0.0, 0.0, 0.1, 0.8, 0.1],\n",
        "    'sep_2023_incidence': [1.5, 1.4, 0.0, 1.5, 0.2, 0.0, 0.0, 0.0, 1.1, 0.1],\n",
        "    'jul_2023_incidence': [3.3, 1.4, 0.0, 0.7, 0.2, 0.0, 0.0, 0.1, 0.4, 0.4]\n",
        "}\n",
        "\n",
        "fake_stamp_df = pd.DataFrame(fake_stamp_data)\n",
        "\n",
        "# 3. Top Illegal Brands by State (Top 3 brands per state)\n",
        "top_brands_data = []\n",
        "states_brands = {\n",
        "    'Perlis': [('John', 32.5), ('Gudang Garam', 4.2), ('Vess', 4.1)],\n",
        "    'Kedah': [('John', 14.3), ('Tiara', 6.0), ('Misto', 3.2)],\n",
        "    'Penang': [('John', 36.4), ('Gudang Garam', 2.6), ('Manchester', 1.5)],\n",
        "    'Perak': [('John', 12.9), ('D&J', 3.9), ('Misto', 3.1)],\n",
        "    'Selangor': [('John', 8.1), ('L.A.', 6.2), ('U2', 5.7)],\n",
        "    'WP KL': [('John', 9.4), ('U2', 7.8), ('L.A.', 6.6)],\n",
        "    'N.Sembilan': [('A380', 9.0), ('Canyon', 7.2), ('Misto', 6.5)],\n",
        "    'Melaka': [('John', 16.3), ('Booston', 13.3), ('Misto', 7.4)],\n",
        "    'Johor': [('Canyon', 10.9), ('Booston', 6.6), ('John', 5.9)],\n",
        "    'Pahang': [('John', 34.7), ('Gudang Garam', 14.9), ('Booston', 13.5)],\n",
        "    'Terengganu': [('John', 41.5), ('Misto', 5.8), ('Saat', 5.5)],\n",
        "    'Kelantan': [('John', 31.4), ('Saat', 25.4), ('Lee', 1.8)],\n",
        "    'Sabah': [('Era', 71.7), ('Premium', 2.9), ('Gudang Garam', 1.7)],\n",
        "    'Sarawak': [('Era', 33.7), ('Parkway', 25.6), ('Rave', 10.2)]\n",
        "}\n",
        "\n",
        "for state, brands in states_brands.items():\n",
        "    for i, (brand, share) in enumerate(brands, 1):\n",
        "        top_brands_data.append({\n",
        "            'state': state,\n",
        "            'brand': brand,\n",
        "            'market_share_percent': share,\n",
        "            'rank': i\n",
        "        })\n",
        "\n",
        "top_brands_df = pd.DataFrame(top_brands_data)\n",
        "\n",
        "# 4. State-level Incidence Trend Table (Multi-Wave)\n",
        "trend_data = []\n",
        "states_trends = {\n",
        "    'Perlis': {'2019': None, '2020': None, '2021': None, '2022': None, '2023': 50.6, 'Sep_2023': 51.8, 'Nov_2023': 51.8, 'Jan_2024': 41.8},\n",
        "    'Kedah': {'2019': None, '2020': None, '2021': None, '2022': None, '2023': 46.4, 'Sep_2023': 43.3, 'Nov_2023': 43.3, 'Jan_2024': 32.4},\n",
        "    'Penang': {'2019': None, '2020': None, '2021': None, '2022': None, '2023': 46.8, 'Sep_2023': 46.5, 'Nov_2023': 46.5, 'Jan_2024': 47.8},\n",
        "    'Perak': {'2019': None, '2020': None, '2021': None, '2022': None, '2023': 34.4, 'Sep_2023': 33.3, 'Nov_2023': 33.3, 'Jan_2024': 33.2},\n",
        "    'Selangor': {'2019': None, '2020': None, '2021': None, '2022': None, '2023': 45.3, 'Sep_2023': 44.1, 'Nov_2023': 44.1, 'Jan_2024': 44.7},\n",
        "    'WP KL': {'2019': None, '2020': None, '2021': None, '2022': None, '2023': 41.4, 'Sep_2023': 42.2, 'Nov_2023': 42.2, 'Jan_2024': 42.1},\n",
        "    'N.Sembilan': {'2019': None, '2020': None, '2021': None, '2022': None, '2023': 41.3, 'Sep_2023': 38.5, 'Nov_2023': 38.5, 'Jan_2024': 42.0},\n",
        "    'Melaka': {'2019': None, '2020': None, '2021': None, '2022': None, '2023': 49.2, 'Sep_2023': 51.1, 'Nov_2023': 51.1, 'Jan_2024': 56.2},\n",
        "    'Johor': {'2019': None, '2020': None, '2021': None, '2022': None, '2023': 56.0, 'Sep_2023': 56.0, 'Nov_2023': 56.0, 'Jan_2024': 57.0},\n",
        "    'Pahang': {'2019': None, '2020': None, '2021': None, '2022': None, '2023': 77.7, 'Sep_2023': 79.8, 'Nov_2023': 79.8, 'Jan_2024': 80.7},\n",
        "    'Terengganu': {'2019': None, '2020': None, '2021': None, '2022': None, '2023': 68.4, 'Sep_2023': 65.7, 'Nov_2023': 65.7, 'Jan_2024': 70.5},\n",
        "    'Kelantan': {'2019': None, '2020': None, '2021': None, '2022': None, '2023': 56.9, 'Sep_2023': 55.1, 'Nov_2023': 55.1, 'Jan_2024': 60.3},\n",
        "    'Sabah': {'2019': 82.8, '2020': 85.3, '2021': 82.1, '2022': 80.3, '2023': 78.4, 'Sep_2023': 77.8, 'Nov_2023': 77.8, 'Jan_2024': 78.9},\n",
        "    'Sarawak': {'2019': 82.8, '2020': 85.3, '2021': 82.1, '2022': 80.3, '2023': 80.2, 'Sep_2023': 79.9, 'Nov_2023': 79.9, 'Jan_2024': 80.3},\n",
        "    'National': {'2019': 62.3, '2020': 63.8, '2021': 57.3, '2022': 56.6, '2023': 55.6, 'Sep_2023': 55.4, 'Nov_2023': 55.3, 'Jan_2024': 56.4}\n",
        "}\n",
        "\n",
        "for state, trends in states_trends.items():\n",
        "    for period, incidence in trends.items():\n",
        "        if incidence is not None:\n",
        "            trend_data.append({\n",
        "                'state': state,\n",
        "                'period': period,\n",
        "                'illegal_incidence_percent': incidence\n",
        "            })\n",
        "\n",
        "trend_df = pd.DataFrame(trend_data)\n",
        "\n",
        "# 5. Illegal Packs by Category Table\n",
        "category_data = {\n",
        "    'category': ['Without Tax Stamp', 'With Fake Tax Stamp', 'Smuggled Whites', 'Smuggled Kreteks'],\n",
        "    'jan_2024_incidence': [45.8, 10.6, 38.5, 7.3],\n",
        "    'nov_2023_incidence': [44.7, 10.6, 38.4, 7.3],\n",
        "    '2023_incidence': [46.9, 8.7, 39.5, 7.4]\n",
        "}\n",
        "\n",
        "category_df = pd.DataFrame(category_data)\n",
        "\n",
        "# 6. Pack-level Fields Reference Table\n",
        "pack_fields_data = {\n",
        "    'field_category': ['Collection Information', 'Collection Information', 'Collection Information', \n",
        "                      'Product Identification', 'Product Identification', 'Product Identification',\n",
        "                      'Manufacturer/Importer', 'Manufacturer/Importer', 'Tax Stamp', 'Tax Stamp',\n",
        "                      'Compliance Checks', 'Compliance Checks', 'Compliance Checks', 'Compliance Checks'],\n",
        "    'field_name': ['State', 'District', 'Collection Date', 'Brand', 'Variant', 'Pack Size & Type',\n",
        "                   'Company Name', 'Address', 'Tax Stamp Type', 'Tax Stamp Color',\n",
        "                   'Health Warning Presence', 'Pictorial Health Warning', 'Manufacturing Date', \n",
        "                   'No Sales to Under 18 Clause'],\n",
        "    'description': ['State where pack was collected', 'District within state', 'Date of collection',\n",
        "                   'Brand name', 'Variant/sub-brand', 'Pack size and type (soft/hard)',\n",
        "                   'Manufacturer or importer company name', 'Company address',\n",
        "                   'Type of tax stamp (blue/pink/none/fake)', 'Color coding for market type',\n",
        "                   'Presence of health warning', 'Compliance with pictorial warning requirements',\n",
        "                   'Manufacturing date on pack', 'Age restriction warning presence'],\n",
        "    'data_type': ['Categorical', 'Categorical', 'Date', 'Categorical', 'Categorical', 'Categorical',\n",
        "                 'Text', 'Text', 'Categorical', 'Categorical', 'Boolean', 'Boolean', 'Date', 'Boolean']\n",
        "}\n",
        "\n",
        "pack_fields_df = pd.DataFrame(pack_fields_data)\n",
        "\n",
        "# Save all dataframes to CSV files\n",
        "market_volume_df.to_csv('data/processed/national_state_market_volume.csv', index=False)\n",
        "fake_stamp_df.to_csv('data/processed/fake_tax_stamp_importers.csv', index=False)\n",
        "top_brands_df.to_csv('data/processed/top_illegal_brands_by_state.csv', index=False)\n",
        "trend_df.to_csv('data/processed/state_incidence_trends.csv', index=False)\n",
        "category_df.to_csv('data/processed/illegal_packs_by_category.csv', index=False)\n",
        "pack_fields_df.to_csv('data/processed/pack_level_fields_reference.csv', index=False)\n",
        "\n",
        "# Display sample of each dataframe\n",
        "print(\"✅ Extraction Complete! Created 6 CSV files:\")\n",
        "print(\"\\n1. national_state_market_volume.csv\")\n",
        "print(market_volume_df.head(3))\n",
        "print(f\"\\nShape: {market_volume_df.shape}\")\n",
        "\n",
        "print(\"\\n2. fake_tax_stamp_importers.csv\")\n",
        "print(fake_stamp_df.head(3))\n",
        "print(f\"\\nShape: {fake_stamp_df.shape}\")\n",
        "\n",
        "print(\"\\n3. top_illegal_brands_by_state.csv\")\n",
        "print(top_brands_df.head(6))\n",
        "print(f\"\\nShape: {top_brands_df.shape}\")\n",
        "\n",
        "print(\"\\n4. state_incidence_trends.csv\")\n",
        "print(trend_df.head(4))\n",
        "print(f\"\\nShape: {trend_df.shape}\")\n",
        "\n",
        "print(\"\\n5. illegal_packs_by_category.csv\")\n",
        "print(category_df.head())\n",
        "print(f\"\\nShape: {category_df.shape}\")\n",
        "\n",
        "print(\"\\n6. pack_level_fields_reference.csv\")\n",
        "print(pack_fields_df.head(4))\n",
        "print(f\"\\nShape: {pack_fields_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellUniqueIdByVincent": "814d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 DATA VALIDATION SUMMARY\n",
            "==================================================\n",
            "National Illegal Sticks: 660,432 (000)\n",
            "National Total Sticks: 1,170,751 (000)\n",
            "National Incidence: 56.4%\n",
            "\n",
            "Total States: 14\n",
            "Total Importers Tracked: 10\n",
            "Total Brand-State Combinations: 42\n",
            "\n",
            "📈 Time Periods Covered:\n",
            "Available periods: ['2023', 'Sep_2023', 'Nov_2023', 'Jan_2024', '2019', '2020', '2021', '2022']\n",
            "\n",
            "🔥 High Incidence States (>70%):\n",
            "  Pahang: 80.7%\n",
            "  Terengganu: 70.5%\n",
            "  Sabah: 78.9%\n",
            "  Sarawak: 80.3%\n",
            "\n",
            "🚨 Top Fake Stamp Importers (Jan 2024):\n",
            "  Mahata Tobacco Marketing: 2.6%\n",
            "  JTM Tobacco Marketing: 2.0%\n",
            "  Milton Tobacco International: 1.2%\n"
          ]
        }
      ],
      "source": [
        "# Create additional summary statistics for validation\n",
        "print(\"📊 DATA VALIDATION SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Validate national totals match report\n",
        "national_row = market_volume_df[market_volume_df['state'] == 'National']\n",
        "print(f\"National Illegal Sticks: {national_row['illegal_sticks_000'].values[0]:,} (000)\")\n",
        "print(f\"National Total Sticks: {national_row['total_sticks_000'].values[0]:,} (000)\")\n",
        "print(f\"National Incidence: {national_row['incidence_percent'].values[0]}%\")\n",
        "\n",
        "print(f\"\\nTotal States: {len(market_volume_df) - 1}\")  # Exclude National\n",
        "print(f\"Total Importers Tracked: {len(fake_stamp_df)}\")\n",
        "print(f\"Total Brand-State Combinations: {len(top_brands_df)}\")\n",
        "\n",
        "# Check data completeness\n",
        "print(f\"\\n📈 Time Periods Covered:\")\n",
        "periods = trend_df['period'].unique()\n",
        "print(f\"Available periods: {list(periods)}\")\n",
        "\n",
        "print(f\"\\n🔥 High Incidence States (>70%):\")\n",
        "high_incidence = market_volume_df[market_volume_df['incidence_percent'] > 70]\n",
        "for _, row in high_incidence.iterrows():\n",
        "    if row['state'] != 'National':\n",
        "        print(f\"  {row['state']}: {row['incidence_percent']}%\")\n",
        "\n",
        "print(f\"\\n🚨 Top Fake Stamp Importers (Jan 2024):\")\n",
        "top_importers = fake_stamp_df.nlargest(3, 'jan_2024_incidence')\n",
        "for _, row in top_importers.iterrows():\n",
        "    print(f\"  {row['importer']}: {row['jan_2024_incidence']}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellUniqueIdByVincent": "4ea98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🎯 SAMPLE PACK-LEVEL DATA (synthetic for demo)\n",
            "   pack_id     state       district collection_date         brand  pack_size  \\\n",
            "0        1     Johor       Petaling      2024-01-01          John         12   \n",
            "1        2     WP KL        Kuching      2024-01-02          John         12   \n",
            "2        3   Sarawak    City Center      2024-01-03           Era         16   \n",
            "3        4     Sabah        Kuching      2024-01-04       Booston         20   \n",
            "4        5  Selangor    City Center      2024-01-05           Era         20   \n",
            "5        6  Selangor    City Center      2024-01-06        Canyon         16   \n",
            "6        7  Selangor  Kota Kinabalu      2024-01-07  Gudang Garam         20   \n",
            "7        8     WP KL    City Center      2024-01-08           Era         20   \n",
            "8        9     Sabah        Kuching      2024-01-09          John         20   \n",
            "9       10   Sarawak    City Center      2024-01-10  Gudang Garam         16   \n",
            "\n",
            "  tax_stamp_type manufacturer  is_illegal  \n",
            "0           Blue       Milton           1  \n",
            "1           Blue       Milton           1  \n",
            "2           None       Milton           1  \n",
            "3           None      Neptune           1  \n",
            "4           None     Legal Co           0  \n",
            "5           None       Milton           0  \n",
            "6           Blue      Neptune           0  \n",
            "7           Blue          JTM           1  \n",
            "8           None          JTM           1  \n",
            "9           Blue      Neptune           1  \n",
            "\n",
            "Shape: (100, 9)\n",
            "Illegal rate in sample: 54.0%\n",
            "\n",
            "============================================================\n",
            "📁 COMPLETE DATA EXTRACTION SUMMARY\n",
            "============================================================\n",
            "Created structured datasets:\n",
            "1. ✅ National & State Market Volume (15 states)\n",
            "2. ✅ Fake Tax Stamp by Importer (10 importers, 4 time periods)\n",
            "3. ✅ Top Illegal Brands by State (42 brand-state combinations)\n",
            "4. ✅ State Incidence Trends (8 time periods, 15 states)\n",
            "5. ✅ Illegal Packs by Category (4 categories, 3 time periods)\n",
            "6. ✅ Pack-level Fields Reference (14 data fields)\n",
            "7. ✅ Sample Pack-level Data (100 synthetic packs)\n",
            "\n",
            "🚀 Ready for Spatio-Temporal Analysis & Market Simulation!\n"
          ]
        }
      ],
      "source": [
        "# Create a sample synthetic pack-level dataset for demonstration\n",
        "# This would simulate the actual 20,400 packs collected per wave\n",
        "\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "# Sample data structure matching the pack_fields reference\n",
        "sample_packs_data = {\n",
        "    'pack_id': range(1, 101),  # Sample 100 packs\n",
        "    'state': np.random.choice(['Selangor', 'Johor', 'Sabah', 'Sarawak', 'WP KL'], 100, p=[0.20, 0.25, 0.20, 0.20, 0.15]),\n",
        "    'district': np.random.choice(['Petaling', 'Johor Bahru', 'Kota Kinabalu', 'Kuching', 'City Center'], 100),\n",
        "    'collection_date': pd.date_range('2024-01-01', periods=100, freq='D'),\n",
        "    'brand': np.random.choice(['John', 'Era', 'Canyon', 'Gudang Garam', 'U2', 'Booston'], 100),\n",
        "    'pack_size': np.random.choice([20, 16, 12], 100, p=[0.85, 0.10, 0.05]),\n",
        "    'tax_stamp_type': np.random.choice(['Blue', 'Pink', 'None', 'Fake'], 100, p=[0.44, 0.01, 0.46, 0.09]),\n",
        "    'manufacturer': np.random.choice(['Mahata', 'JTM', 'Neptune', 'Milton', 'Legal Co'], 100),\n",
        "    'is_illegal': np.random.choice([0, 1], 100, p=[0.44, 0.56])  # 56% illegal based on national incidence\n",
        "}\n",
        "\n",
        "sample_packs_df = pd.DataFrame(sample_packs_data)\n",
        "sample_packs_df.to_csv('data/processed/sample_pack_level_data.csv', index=False)\n",
        "\n",
        "print(\"\\n🎯 SAMPLE PACK-LEVEL DATA (synthetic for demo)\")\n",
        "print(sample_packs_df.head(10))\n",
        "print(f\"\\nShape: {sample_packs_df.shape}\")\n",
        "print(f\"Illegal rate in sample: {sample_packs_df['is_illegal'].mean():.1%}\")\n",
        "\n",
        "# Summary of all created files\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📁 COMPLETE DATA EXTRACTION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(\"Created structured datasets:\")\n",
        "print(\"1. ✅ National & State Market Volume (15 states)\")\n",
        "print(\"2. ✅ Fake Tax Stamp by Importer (10 importers, 4 time periods)\")\n",
        "print(\"3. ✅ Top Illegal Brands by State (42 brand-state combinations)\")\n",
        "print(\"4. ✅ State Incidence Trends (8 time periods, 15 states)\")\n",
        "print(\"5. ✅ Illegal Packs by Category (4 categories, 3 time periods)\")\n",
        "print(\"6. ✅ Pack-level Fields Reference (14 data fields)\")\n",
        "print(\"7. ✅ Sample Pack-level Data (100 synthetic packs)\")\n",
        "print(\"\\n🚀 Ready for Spatio-Temporal Analysis & Market Simulation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellUniqueIdByVincent": "25fba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🗺️ SPATIO-TEMPORAL SURVEILLANCE ANALYSIS\n",
            "============================================================\n",
            "\n",
            "🔥 HOTSPOT IDENTIFICATION\n",
            "------------------------------\n",
            "State Risk Classification:\n",
            "            state incidence_percent             illegal_sticks_000\n",
            "            count              mean   min   max                sum\n",
            "risk_level                                                        \n",
            "HIGH RISK       4             77.60  70.5  80.7             254027\n",
            "LOW RISK        7             40.57  32.4  47.8             228808\n",
            "MEDIUM RISK     3             57.83  56.2  60.3             177597\n",
            "\n",
            "🚨 HIGH RISK STATES (>70% illegal incidence):\n",
            "  Pahang      :  80.7% | 92,479 (000 sticks)\n",
            "  Sarawak     :  80.3% | 66,596 (000 sticks)\n",
            "  Sabah       :  78.9% | 70,680 (000 sticks)\n",
            "  Terengganu  :  70.5% | 24,272 (000 sticks)\n",
            "\n",
            "Total illegal volume in HIGH RISK states: 254,027 (000 sticks)\n",
            "Percentage of national illegal volume: 38.5%\n"
          ]
        }
      ],
      "source": [
        "# SPATIO-TEMPORAL SURVEILLANCE & HOTSPOT DETECTION\n",
        "# Phase 1: Exploratory Spatial Data Analysis\n",
        "\n",
        "print(\"🗺️ SPATIO-TEMPORAL SURVEILLANCE ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load the processed datasets\n",
        "market_volume_df = pd.read_csv('data/processed/national_state_market_volume.csv')\n",
        "trend_df = pd.read_csv('data/processed/state_incidence_trends.csv')\n",
        "top_brands_df = pd.read_csv('data/processed/top_illegal_brands_by_state.csv')\n",
        "fake_stamp_df = pd.read_csv('data/processed/fake_tax_stamp_importers.csv')\n",
        "\n",
        "# 1. HOTSPOT IDENTIFICATION\n",
        "print(\"\\n🔥 HOTSPOT IDENTIFICATION\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Define hotspot criteria\n",
        "HIGH_INCIDENCE_THRESHOLD = 70.0  # States with >70% illegal incidence\n",
        "MEDIUM_INCIDENCE_THRESHOLD = 50.0  # States with 50-70% illegal incidence\n",
        "\n",
        "# Classify states by risk level\n",
        "def classify_risk_level(incidence):\n",
        "    if incidence >= HIGH_INCIDENCE_THRESHOLD:\n",
        "        return 'HIGH RISK'\n",
        "    elif incidence >= MEDIUM_INCIDENCE_THRESHOLD:\n",
        "        return 'MEDIUM RISK'\n",
        "    else:\n",
        "        return 'LOW RISK'\n",
        "\n",
        "# Add risk classification to market volume data\n",
        "market_volume_df['risk_level'] = market_volume_df['incidence_percent'].apply(classify_risk_level)\n",
        "\n",
        "# Display hotspot analysis\n",
        "print(\"State Risk Classification:\")\n",
        "risk_summary = market_volume_df[market_volume_df['state'] != 'National'].groupby('risk_level').agg({\n",
        "    'state': 'count',\n",
        "    'incidence_percent': ['mean', 'min', 'max'],\n",
        "    'illegal_sticks_000': 'sum'\n",
        "}).round(2)\n",
        "\n",
        "print(risk_summary)\n",
        "\n",
        "print(\"\\n🚨 HIGH RISK STATES (>70% illegal incidence):\")\n",
        "high_risk_states = market_volume_df[\n",
        "    (market_volume_df['incidence_percent'] >= HIGH_INCIDENCE_THRESHOLD) & \n",
        "    (market_volume_df['state'] != 'National')\n",
        "].sort_values('incidence_percent', ascending=False)\n",
        "\n",
        "for _, state in high_risk_states.iterrows():\n",
        "    print(f\"  {state['state']:12s}: {state['incidence_percent']:5.1f}% | {state['illegal_sticks_000']:,} (000 sticks)\")\n",
        "\n",
        "print(f\"\\nTotal illegal volume in HIGH RISK states: {high_risk_states['illegal_sticks_000'].sum():,} (000 sticks)\")\n",
        "print(f\"Percentage of national illegal volume: {(high_risk_states['illegal_sticks_000'].sum() / 660432 * 100):.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellUniqueIdByVincent": "dfab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📈 TEMPORAL TREND ANALYSIS\n",
            "------------------------------\n",
            "Recent Wave-over-Wave Changes (percentage points):\n",
            "State           Sep→Nov  Nov→Jan  Annual(2023→Jan2024)\n",
            "-------------------------------------------------------\n",
            "Johor             0.0      1.0      1.0\n",
            "Kedah             0.0    -10.9    -14.0\n",
            "Kelantan          0.0      5.2      3.4\n",
            "Melaka            0.0      5.1      7.0\n",
            "N.Sembilan        0.0      3.5      0.7\n",
            "Pahang            0.0      0.9      3.0\n",
            "Penang            0.0      1.3      1.0\n",
            "Perak             0.0     -0.1     -1.2\n",
            "Perlis            0.0    -10.0     -8.8\n",
            "Sabah             0.0      1.1      0.5\n",
            "Sarawak           0.0      0.4      0.1\n",
            "Selangor          0.0      0.6     -0.6\n",
            "Terengganu        0.0      4.8      2.1\n",
            "WP KL             0.0     -0.1      0.7\n",
            "\n",
            "🚨 SIGNIFICANT CHANGES DETECTED:\n",
            "  Kedah       : -14.0pp ↘️ DECREASE\n",
            "  Perlis      :  -8.8pp ↘️ DECREASE\n",
            "  Melaka      :  +7.0pp ↗️ INCREASE\n",
            "\n",
            "🔮 FORECASTING NEXT WAVE (Apr 2024 Projection)\n",
            "---------------------------------------------\n",
            "State           Jan 2024  Apr 2024  Change\n",
            "                Actual   Forecast   (pp)\n",
            "---------------------------------------------\n",
            "Pahang           80.7     81.2    +0.5\n",
            "Sarawak          80.3     80.5    +0.2\n",
            "Sabah            78.9     79.5    +0.6\n",
            "Terengganu       70.5     72.9    +2.4\n",
            "Kelantan         60.3     62.9    +2.6\n",
            "Melaka           56.2     58.8    +2.5\n",
            "Johor            57.0     57.5    +0.5\n",
            "Penang           47.8     48.4    +0.6\n",
            "Selangor         44.7     45.0    +0.3\n",
            "N.Sembilan       42.0     43.8    +1.8\n",
            "\n",
            "💾 Saved forecasts to: data/processed/state_forecasts_apr2024.csv\n"
          ]
        }
      ],
      "source": [
        "# 2. TEMPORAL TREND ANALYSIS & FORECASTING\n",
        "print(\"\\n📈 TEMPORAL TREND ANALYSIS\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Prepare time series data\n",
        "trend_pivot = trend_df.pivot(index='state', columns='period', values='illegal_incidence_percent')\n",
        "\n",
        "# Calculate wave-over-wave changes for recent periods\n",
        "recent_periods = ['2023', 'Sep_2023', 'Nov_2023', 'Jan_2024']\n",
        "trend_recent = trend_pivot[recent_periods].copy()\n",
        "\n",
        "# Calculate changes\n",
        "trend_recent['Sep_to_Nov_change'] = trend_recent['Nov_2023'] - trend_recent['Sep_2023']\n",
        "trend_recent['Nov_to_Jan_change'] = trend_recent['Jan_2024'] - trend_recent['Nov_2023']\n",
        "trend_recent['Annual_change_2023'] = trend_recent['Jan_2024'] - trend_recent['2023']\n",
        "\n",
        "print(\"Recent Wave-over-Wave Changes (percentage points):\")\n",
        "print(\"State           Sep→Nov  Nov→Jan  Annual(2023→Jan2024)\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "for state in trend_recent.index:\n",
        "    if state != 'National':\n",
        "        sep_nov = trend_recent.loc[state, 'Sep_to_Nov_change']\n",
        "        nov_jan = trend_recent.loc[state, 'Nov_to_Jan_change']\n",
        "        annual = trend_recent.loc[state, 'Annual_change_2023']\n",
        "        \n",
        "        if pd.notna(sep_nov) and pd.notna(nov_jan) and pd.notna(annual):\n",
        "            print(f\"{state:12s}   {sep_nov:6.1f}   {nov_jan:6.1f}   {annual:6.1f}\")\n",
        "\n",
        "# Identify states with significant changes (>5 percentage points)\n",
        "print(\"\\n🚨 SIGNIFICANT CHANGES DETECTED:\")\n",
        "significant_changes = []\n",
        "for state in trend_recent.index:\n",
        "    if state != 'National':\n",
        "        annual_change = trend_recent.loc[state, 'Annual_change_2023']\n",
        "        if pd.notna(annual_change) and abs(annual_change) > 5:\n",
        "            direction = \"↗️ INCREASE\" if annual_change > 0 else \"↘️ DECREASE\"\n",
        "            significant_changes.append((state, annual_change, direction))\n",
        "\n",
        "significant_changes.sort(key=lambda x: abs(x[1]), reverse=True)\n",
        "for state, change, direction in significant_changes:\n",
        "    print(f\"  {state:12s}: {change:+5.1f}pp {direction}\")\n",
        "\n",
        "# 3. SIMPLE FORECASTING MODEL\n",
        "print(\"\\n🔮 FORECASTING NEXT WAVE (Apr 2024 Projection)\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "# Simple trend-based forecasting using recent 3-wave average\n",
        "forecast_data = []\n",
        "for state in trend_recent.index:\n",
        "    if state != 'National':\n",
        "        recent_values = [trend_recent.loc[state, 'Sep_2023'], \n",
        "                        trend_recent.loc[state, 'Nov_2023'], \n",
        "                        trend_recent.loc[state, 'Jan_2024']]\n",
        "        \n",
        "        # Remove NaN values\n",
        "        recent_values = [v for v in recent_values if pd.notna(v)]\n",
        "        \n",
        "        if len(recent_values) >= 2:\n",
        "            # Simple linear trend\n",
        "            if len(recent_values) == 3:\n",
        "                trend_slope = (recent_values[2] - recent_values[0]) / 2  # Change over 2 periods\n",
        "                forecast = recent_values[2] + trend_slope\n",
        "            else:\n",
        "                forecast = recent_values[-1]  # Use last available value\n",
        "            \n",
        "            # Ensure forecast is within reasonable bounds\n",
        "            forecast = max(0, min(100, forecast))\n",
        "            \n",
        "            forecast_data.append({\n",
        "                'state': state,\n",
        "                'jan_2024_actual': recent_values[-1],\n",
        "                'apr_2024_forecast': forecast,\n",
        "                'forecast_change': forecast - recent_values[-1]\n",
        "            })\n",
        "\n",
        "forecast_df = pd.DataFrame(forecast_data)\n",
        "forecast_df = forecast_df.sort_values('apr_2024_forecast', ascending=False)\n",
        "\n",
        "print(\"State           Jan 2024  Apr 2024  Change\")\n",
        "print(\"                Actual   Forecast   (pp)\")\n",
        "print(\"-\" * 45)\n",
        "for _, row in forecast_df.head(10).iterrows():\n",
        "    print(f\"{row['state']:12s}   {row['jan_2024_actual']:6.1f}   {row['apr_2024_forecast']:6.1f}   {row['forecast_change']:+5.1f}\")\n",
        "\n",
        "# Save forecasting results\n",
        "forecast_df.to_csv('data/processed/state_forecasts_apr2024.csv', index=False)\n",
        "print(f\"\\n💾 Saved forecasts to: data/processed/state_forecasts_apr2024.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellUniqueIdByVincent": "5a405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💰 NATIONAL MARKET SIMULATION & ECONOMIC ANALYSIS\n",
            "============================================================\n",
            "\n",
            "💸 ECONOMIC LOSS ESTIMATION\n",
            "------------------------------\n",
            "ECONOMIC IMPACT BY STATE (Top 10):\n",
            "State           Illegal    Excise     Total Tax   Per Capita\n",
            "                Sticks     Loss       Loss        Loss\n",
            "                (000)      (RM 000)   (RM 000)    (RM)\n",
            "-----------------------------------------------------------------\n",
            "Johor          132262      52905      58195     15.4\n",
            "Selangor       102534      41014      45115      6.9\n",
            "Pahang          92479      36992      40691     24.1\n",
            "Sabah           70680      28272      31099      9.1\n",
            "Sarawak         66596      26638      29302     11.9\n",
            "WP KL           43776      17510      19261      9.7\n",
            "Penang          33129      13252      14577      8.2\n",
            "Terengganu      24272       9709      10680      8.6\n",
            "Melaka          23713       9485      10434     11.2\n",
            "Kelantan        21622       8649       9514      5.0\n",
            "\n",
            "📊 NATIONAL ECONOMIC IMPACT:\n",
            "Total Illegal Sticks: 660,432 (000)\n",
            "Total Excise Loss: RM 264,173 (000)\n",
            "Total Tax Loss: RM 290,590 (000)\n",
            "Daily Tax Loss: RM 796,137\n"
          ]
        }
      ],
      "source": [
        "# NATIONAL MARKET SIMULATION & ECONOMIC LOSS ESTIMATION\n",
        "print(\"\\n💰 NATIONAL MARKET SIMULATION & ECONOMIC ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 4. ECONOMIC LOSS CALCULATION\n",
        "print(\"\\n💸 ECONOMIC LOSS ESTIMATION\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Excise tax rates (estimated based on Malaysian tobacco taxation)\n",
        "# Note: These are estimated values - actual rates should be sourced from official data\n",
        "EXCISE_RATE_PER_STICK = 0.40  # RM per stick (estimated)\n",
        "SALES_TAX_RATE = 0.10  # 10% sales tax\n",
        "IMPORT_DUTY_RATE = 0.25  # 25% import duty (estimated)\n",
        "\n",
        "# Calculate economic losses by state\n",
        "economic_analysis = market_volume_df[market_volume_df['state'] != 'National'].copy()\n",
        "\n",
        "# Calculate revenue losses\n",
        "economic_analysis['excise_loss_rm_000'] = economic_analysis['illegal_sticks_000'] * EXCISE_RATE_PER_STICK\n",
        "economic_analysis['sales_tax_loss_rm_000'] = economic_analysis['excise_loss_rm_000'] * SALES_TAX_RATE\n",
        "economic_analysis['total_tax_loss_rm_000'] = economic_analysis['excise_loss_rm_000'] + economic_analysis['sales_tax_loss_rm_000']\n",
        "\n",
        "# Calculate per capita losses (using estimated state populations)\n",
        "state_populations = {\n",
        "    'Perlis': 255, 'Kedah': 2178, 'Penang': 1767, 'Perak': 2507, 'Selangor': 6538,\n",
        "    'WP KL': 1982, 'N.Sembilan': 1129, 'Melaka': 932, 'Johor': 3781, 'Pahang': 1691,\n",
        "    'Terengganu': 1249, 'Kelantan': 1906, 'Sabah': 3418, 'Sarawak': 2453\n",
        "}\n",
        "\n",
        "economic_analysis['population_000'] = economic_analysis['state'].map(state_populations)\n",
        "economic_analysis['tax_loss_per_capita_rm'] = economic_analysis['total_tax_loss_rm_000'] / economic_analysis['population_000']\n",
        "\n",
        "# Sort by total tax loss\n",
        "economic_analysis = economic_analysis.sort_values('total_tax_loss_rm_000', ascending=False)\n",
        "\n",
        "print(\"ECONOMIC IMPACT BY STATE (Top 10):\")\n",
        "print(\"State           Illegal    Excise     Total Tax   Per Capita\")\n",
        "print(\"                Sticks     Loss       Loss        Loss\")\n",
        "print(\"                (000)      (RM 000)   (RM 000)    (RM)\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for _, row in economic_analysis.head(10).iterrows():\n",
        "    print(f\"{row['state']:12s}   {row['illegal_sticks_000']:6.0f}   {row['excise_loss_rm_000']:8.0f}   {row['total_tax_loss_rm_000']:8.0f}   {row['tax_loss_per_capita_rm']:6.1f}\")\n",
        "\n",
        "# National totals\n",
        "national_illegal_sticks = economic_analysis['illegal_sticks_000'].sum()\n",
        "national_excise_loss = economic_analysis['excise_loss_rm_000'].sum()\n",
        "national_total_loss = economic_analysis['total_tax_loss_rm_000'].sum()\n",
        "\n",
        "print(f\"\\n📊 NATIONAL ECONOMIC IMPACT:\")\n",
        "print(f\"Total Illegal Sticks: {national_illegal_sticks:,.0f} (000)\")\n",
        "print(f\"Total Excise Loss: RM {national_excise_loss:,.0f} (000)\")\n",
        "print(f\"Total Tax Loss: RM {national_total_loss:,.0f} (000)\")\n",
        "print(f\"Daily Tax Loss: RM {national_total_loss * 1000 / 365:,.0f}\")\n",
        "\n",
        "# Save economic analysis\n",
        "economic_analysis.to_csv('data/processed/economic_impact_by_state.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellUniqueIdByVincent": "1941c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔬 ADVANCED SPATIAL CLUSTERING ANALYSIS\n",
            "============================================================\n",
            "\n",
            "📍 SPATIAL AUTOCORRELATION ANALYSIS\n",
            "----------------------------------------\n",
            "Spatial Weights Matrix (first 5x5):\n",
            "[[0.    0.417 0.147 0.074 0.042]\n",
            " [0.373 0.    0.194 0.081 0.042]\n",
            " [0.158 0.232 0.    0.146 0.062]\n",
            " [0.082 0.1   0.151 0.    0.107]\n",
            " [0.024 0.027 0.033 0.055 0.   ]]\n",
            "\n",
            "📊 GLOBAL MORAN'S I RESULTS:\n",
            "Moran's I for Illegal Incidence: 0.1320\n",
            "✅ Positive spatial autocorrelation detected (clustering)\n",
            "\n",
            "🎯 DBSCAN CLUSTERING ANALYSIS\n",
            "-----------------------------------\n",
            "DBSCAN Clustering Results:\n",
            "Number of clusters: 3\n",
            "Number of noise points: 6\n",
            "Cluster 0: ['Perlis', 'Kedah', 'Penang'] (Avg incidence: 40.7%)\n",
            "Cluster 1: ['Selangor', 'WP KL', 'N.Sembilan'] (Avg incidence: 42.9%)\n",
            "Cluster 2: ['Melaka', 'Johor'] (Avg incidence: 56.6%)\n",
            "Noise points: ['Perak', 'Pahang', 'Terengganu', 'Kelantan', 'Sabah', 'Sarawak']\n",
            "\n",
            "💾 Saved spatial analysis to: data/processed/spatial_clustering_results.csv\n"
          ]
        }
      ],
      "source": [
        "# ADVANCED SPATIAL CLUSTERING ANALYSIS\n",
        "print(\"\\n🔬 ADVANCED SPATIAL CLUSTERING ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 5. SPATIAL AUTOCORRELATION & CLUSTERING\n",
        "print(\"\\n📍 SPATIAL AUTOCORRELATION ANALYSIS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Create synthetic coordinates for Malaysian states (approximate centroids)\n",
        "# In real implementation, use actual state boundary shapefiles\n",
        "state_coordinates = {\n",
        "    'Perlis': (6.4414, 100.1986),\n",
        "    'Kedah': (6.1184, 100.3685),\n",
        "    'Penang': (5.4164, 100.3327),\n",
        "    'Perak': (4.5921, 101.0901),\n",
        "    'Selangor': (3.0738, 101.5183),\n",
        "    'WP KL': (3.1390, 101.6869),\n",
        "    'N.Sembilan': (2.7258, 101.9424),\n",
        "    'Melaka': (2.1896, 102.2501),\n",
        "    'Johor': (1.4927, 103.7414),\n",
        "    'Pahang': (3.8126, 103.3256),\n",
        "    'Terengganu': (5.3117, 103.1324),\n",
        "    'Kelantan': (6.1254, 102.2386),\n",
        "    'Sabah': (5.9804, 116.0735),\n",
        "    'Sarawak': (1.5533, 110.3592)\n",
        "}\n",
        "\n",
        "# Create spatial dataset\n",
        "spatial_data = []\n",
        "for _, row in market_volume_df.iterrows():\n",
        "    if row['state'] != 'National' and row['state'] in state_coordinates:\n",
        "        lat, lon = state_coordinates[row['state']]\n",
        "        spatial_data.append({\n",
        "            'state': row['state'],\n",
        "            'latitude': lat,\n",
        "            'longitude': lon,\n",
        "            'illegal_incidence': row['incidence_percent'],\n",
        "            'illegal_sticks_000': row['illegal_sticks_000'],\n",
        "            'risk_level': row['risk_level']\n",
        "        })\n",
        "\n",
        "spatial_df = pd.DataFrame(spatial_data)\n",
        "\n",
        "# Calculate distance matrix for spatial weights\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "coords = spatial_df[['latitude', 'longitude']].values\n",
        "distance_matrix = squareform(pdist(coords, metric='euclidean'))\n",
        "\n",
        "# Create spatial weights matrix (inverse distance)\n",
        "spatial_weights = np.zeros_like(distance_matrix)\n",
        "for i in range(len(distance_matrix)):\n",
        "    for j in range(len(distance_matrix)):\n",
        "        if i != j and distance_matrix[i, j] > 0:\n",
        "            spatial_weights[i, j] = 1 / distance_matrix[i, j]\n",
        "\n",
        "# Normalize weights\n",
        "row_sums = spatial_weights.sum(axis=1)\n",
        "for i in range(len(spatial_weights)):\n",
        "    if row_sums[i] > 0:\n",
        "        spatial_weights[i] = spatial_weights[i] / row_sums[i]\n",
        "\n",
        "print(\"Spatial Weights Matrix (first 5x5):\")\n",
        "print(spatial_weights[:5, :5].round(3))\n",
        "\n",
        "# Calculate Global Moran's I\n",
        "def calculate_morans_i(values, weights):\n",
        "    \"\"\"Calculate Global Moran's I statistic\"\"\"\n",
        "    n = len(values)\n",
        "    mean_val = np.mean(values)\n",
        "    \n",
        "    # Numerator: sum of spatial autocovariance\n",
        "    numerator = 0\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            numerator += weights[i, j] * (values[i] - mean_val) * (values[j] - mean_val)\n",
        "    \n",
        "    # Denominator: variance * sum of weights\n",
        "    variance = np.sum((values - mean_val) ** 2) / n\n",
        "    weight_sum = np.sum(weights)\n",
        "    \n",
        "    if weight_sum > 0 and variance > 0:\n",
        "        morans_i = numerator / (weight_sum * variance)\n",
        "    else:\n",
        "        morans_i = 0\n",
        "    \n",
        "    return morans_i\n",
        "\n",
        "# Calculate Moran's I for illegal incidence\n",
        "incidence_values = spatial_df['illegal_incidence'].values\n",
        "morans_i = calculate_morans_i(incidence_values, spatial_weights)\n",
        "\n",
        "print(f\"\\n📊 GLOBAL MORAN'S I RESULTS:\")\n",
        "print(f\"Moran's I for Illegal Incidence: {morans_i:.4f}\")\n",
        "if morans_i > 0:\n",
        "    print(\"✅ Positive spatial autocorrelation detected (clustering)\")\n",
        "elif morans_i < 0:\n",
        "    print(\"⚠️ Negative spatial autocorrelation detected (dispersion)\")\n",
        "else:\n",
        "    print(\"➖ No spatial autocorrelation detected (random)\")\n",
        "\n",
        "# DBSCAN Clustering Analysis\n",
        "print(f\"\\n🎯 DBSCAN CLUSTERING ANALYSIS\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Prepare features for clustering\n",
        "features = spatial_df[['latitude', 'longitude', 'illegal_incidence']].copy()\n",
        "features['illegal_incidence_scaled'] = features['illegal_incidence'] / 100  # Scale to 0-1\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features[['latitude', 'longitude', 'illegal_incidence_scaled']])\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.8, min_samples=2)\n",
        "clusters = dbscan.fit_predict(features_scaled)\n",
        "\n",
        "# Add cluster labels to spatial data\n",
        "spatial_df['cluster'] = clusters\n",
        "\n",
        "print(\"DBSCAN Clustering Results:\")\n",
        "print(f\"Number of clusters: {len(set(clusters)) - (1 if -1 in clusters else 0)}\")\n",
        "print(f\"Number of noise points: {list(clusters).count(-1)}\")\n",
        "\n",
        "# Display clusters\n",
        "for cluster_id in set(clusters):\n",
        "    cluster_states = spatial_df[spatial_df['cluster'] == cluster_id]['state'].tolist()\n",
        "    if cluster_id == -1:\n",
        "        print(f\"Noise points: {cluster_states}\")\n",
        "    else:\n",
        "        avg_incidence = spatial_df[spatial_df['cluster'] == cluster_id]['illegal_incidence'].mean()\n",
        "        print(f\"Cluster {cluster_id}: {cluster_states} (Avg incidence: {avg_incidence:.1f}%)\")\n",
        "\n",
        "# Save spatial analysis results\n",
        "spatial_df.to_csv('data/processed/spatial_clustering_results.csv', index=False)\n",
        "print(f\"\\n💾 Saved spatial analysis to: data/processed/spatial_clustering_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellUniqueIdByVincent": "bf270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📈 ADVANCED TIME SERIES FORECASTING\n",
            "============================================================\n",
            "\n",
            "🔮 PROPHET TIME SERIES FORECASTING\n",
            "----------------------------------------\n",
            "Prophet Forecasting Results:\n",
            "State           Current   Apr 2024  Jul 2024  Trend\n",
            "                (Jan24)   Forecast  Forecast\n",
            "-------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "16:32:23 - cmdstanpy - INFO - Chain [1] start processing\n",
            "16:32:25 - cmdstanpy - INFO - Chain [1] done processing\n",
            "16:32:25 - cmdstanpy - INFO - Chain [1] start processing\n",
            "16:32:25 - cmdstanpy - INFO - Chain [1] done processing\n",
            "16:32:25 - cmdstanpy - INFO - Chain [1] start processing\n",
            "16:32:25 - cmdstanpy - INFO - Chain [1] done processing\n",
            "16:32:25 - cmdstanpy - INFO - Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pahang           80.7     89.4     98.3   ↗️\n",
            "Sabah            78.9     80.5     82.1   ↗️\n",
            "Sarawak          80.3     80.9     81.5   ↗️\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "16:32:25 - cmdstanpy - INFO - Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terengganu       70.5     76.6     82.8   ↗️\n",
            "\n",
            "📊 ARIMA FORECASTING - NATIONAL TRENDS\n",
            "----------------------------------------\n",
            "Best ARIMA Model: (0, 1, 0)\n",
            "AIC: 35.23\n",
            "\n",
            "National Incidence Forecasts:\n",
            "Period          Forecast   Lower CI   Upper CI\n",
            "---------------------------------------------\n",
            "Apr 2024         56.4%     51.3%     61.5%\n",
            "Jul 2024         56.4%     49.2%     63.6%\n",
            "Oct 2024         56.4%     47.6%     65.2%\n",
            "\n",
            "💾 Saved advanced forecasts to: data/processed/advanced_forecasts.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/opt/anaconda3/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        }
      ],
      "source": [
        "# ADVANCED TIME SERIES FORECASTING\n",
        "print(\"\\n📈 ADVANCED TIME SERIES FORECASTING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 6. PROPHET & ARIMA FORECASTING MODELS\n",
        "print(\"\\n🔮 PROPHET TIME SERIES FORECASTING\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Prepare time series data for Prophet\n",
        "def prepare_prophet_data(state_name):\n",
        "    \"\"\"Prepare data for Prophet forecasting for a specific state\"\"\"\n",
        "    state_data = trend_df[trend_df['state'] == state_name].copy()\n",
        "    \n",
        "    # Convert period to datetime\n",
        "    period_mapping = {\n",
        "        '2019': '2019-12-31',\n",
        "        '2020': '2020-12-31', \n",
        "        '2021': '2021-12-31',\n",
        "        '2022': '2022-12-31',\n",
        "        '2023': '2023-12-31',\n",
        "        'Sep_2023': '2023-09-30',\n",
        "        'Nov_2023': '2023-11-30',\n",
        "        'Jan_2024': '2024-01-31'\n",
        "    }\n",
        "    \n",
        "    state_data['ds'] = state_data['period'].map(period_mapping)\n",
        "    state_data['ds'] = pd.to_datetime(state_data['ds'])\n",
        "    state_data['y'] = state_data['illegal_incidence_percent']\n",
        "    \n",
        "    return state_data[['ds', 'y']].dropna().sort_values('ds')\n",
        "\n",
        "# Forecast for high-risk states using Prophet\n",
        "prophet_forecasts = {}\n",
        "high_risk_states_list = ['Pahang', 'Sabah', 'Sarawak', 'Terengganu']\n",
        "\n",
        "print(\"Prophet Forecasting Results:\")\n",
        "print(\"State           Current   Apr 2024  Jul 2024  Trend\")\n",
        "print(\"                (Jan24)   Forecast  Forecast\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "for state in high_risk_states_list:\n",
        "    try:\n",
        "        # Prepare data\n",
        "        prophet_data = prepare_prophet_data(state)\n",
        "        \n",
        "        if len(prophet_data) >= 3:  # Need minimum data points\n",
        "            # Create and fit Prophet model\n",
        "            model = Prophet(\n",
        "                yearly_seasonality=False,\n",
        "                weekly_seasonality=False,\n",
        "                daily_seasonality=False,\n",
        "                changepoint_prior_scale=0.1\n",
        "            )\n",
        "            model.fit(prophet_data)\n",
        "            \n",
        "            # Create future dataframe\n",
        "            future = model.make_future_dataframe(periods=6, freq='M')\n",
        "            forecast = model.predict(future)\n",
        "            \n",
        "            # Extract forecasts\n",
        "            current_val = prophet_data['y'].iloc[-1]\n",
        "            apr_forecast = forecast[forecast['ds'] == '2024-04-30']['yhat'].values\n",
        "            jul_forecast = forecast[forecast['ds'] == '2024-07-31']['yhat'].values\n",
        "            \n",
        "            if len(apr_forecast) > 0 and len(jul_forecast) > 0:\n",
        "                apr_val = max(0, min(100, apr_forecast[0]))\n",
        "                jul_val = max(0, min(100, jul_forecast[0]))\n",
        "                trend_direction = \"↗️\" if jul_val > current_val else \"↘️\" if jul_val < current_val else \"→\"\n",
        "                \n",
        "                print(f\"{state:12s}   {current_val:6.1f}   {apr_val:6.1f}   {jul_val:6.1f}   {trend_direction}\")\n",
        "                \n",
        "                prophet_forecasts[state] = {\n",
        "                    'current': current_val,\n",
        "                    'apr_2024': apr_val,\n",
        "                    'jul_2024': jul_val,\n",
        "                    'trend': trend_direction\n",
        "                }\n",
        "    except Exception as e:\n",
        "        print(f\"{state:12s}   Error: {str(e)[:20]}\")\n",
        "\n",
        "# 7. ARIMA FORECASTING FOR NATIONAL TRENDS\n",
        "print(f\"\\n📊 ARIMA FORECASTING - NATIONAL TRENDS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Prepare national time series data\n",
        "national_data = trend_df[trend_df['state'] == 'National'].copy()\n",
        "national_data = national_data.sort_values('period')\n",
        "\n",
        "# Create time index for ARIMA\n",
        "time_periods = ['2019', '2020', '2021', '2022', '2023', 'Sep_2023', 'Nov_2023', 'Jan_2024']\n",
        "national_values = []\n",
        "for period in time_periods:\n",
        "    value = national_data[national_data['period'] == period]['illegal_incidence_percent'].values\n",
        "    if len(value) > 0:\n",
        "        national_values.append(value[0])\n",
        "    else:\n",
        "        national_values.append(np.nan)\n",
        "\n",
        "# Remove NaN values and create time series\n",
        "national_ts = pd.Series([v for v in national_values if not pd.isna(v)])\n",
        "\n",
        "if len(national_ts) >= 4:\n",
        "    try:\n",
        "        # Fit ARIMA model\n",
        "        from statsmodels.tsa.arima.model import ARIMA\n",
        "        \n",
        "        # Try different ARIMA parameters\n",
        "        best_aic = float('inf')\n",
        "        best_model = None\n",
        "        \n",
        "        for p in range(3):\n",
        "            for d in range(2):\n",
        "                for q in range(3):\n",
        "                    try:\n",
        "                        model = ARIMA(national_ts, order=(p, d, q))\n",
        "                        fitted_model = model.fit()\n",
        "                        if fitted_model.aic < best_aic:\n",
        "                            best_aic = fitted_model.aic\n",
        "                            best_model = fitted_model\n",
        "                    except:\n",
        "                        continue\n",
        "        \n",
        "        if best_model is not None:\n",
        "            # Generate forecasts\n",
        "            forecast_steps = 3\n",
        "            forecast = best_model.forecast(steps=forecast_steps)\n",
        "            forecast_ci = best_model.get_forecast(steps=forecast_steps).conf_int()\n",
        "            \n",
        "            print(f\"Best ARIMA Model: {best_model.model.order}\")\n",
        "            print(f\"AIC: {best_aic:.2f}\")\n",
        "            print(\"\\nNational Incidence Forecasts:\")\n",
        "            print(\"Period          Forecast   Lower CI   Upper CI\")\n",
        "            print(\"-\" * 45)\n",
        "            \n",
        "            periods = ['Apr 2024', 'Jul 2024', 'Oct 2024']\n",
        "            for i, period in enumerate(periods):\n",
        "                fc_val = max(0, min(100, forecast.iloc[i]))\n",
        "                lower_ci = max(0, min(100, forecast_ci.iloc[i, 0]))\n",
        "                upper_ci = max(0, min(100, forecast_ci.iloc[i, 1]))\n",
        "                print(f\"{period:12s}   {fc_val:6.1f}%   {lower_ci:6.1f}%   {upper_ci:6.1f}%\")\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"ARIMA modeling error: {e}\")\n",
        "else:\n",
        "    print(\"Insufficient data for ARIMA modeling\")\n",
        "\n",
        "# Save forecasting results\n",
        "forecast_results = {\n",
        "    'prophet_forecasts': prophet_forecasts,\n",
        "    'national_current': national_ts.iloc[-1] if len(national_ts) > 0 else None\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('data/processed/advanced_forecasts.json', 'w') as f:\n",
        "    json.dump(forecast_results, f, indent=2)\n",
        "\n",
        "print(f\"\\n💾 Saved advanced forecasts to: data/processed/advanced_forecasts.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellUniqueIdByVincent": "6aab1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🎲 MONTE CARLO SIMULATION - ENFORCEMENT ROI ANALYSIS\n",
            "============================================================\n",
            "\n",
            "💼 ENFORCEMENT SCENARIO MODELING\n",
            "----------------------------------------\n",
            "Running Monte Carlo simulations...\n",
            "  Simulating Low Intensity scenario...\n",
            "  Simulating Medium Intensity scenario...\n",
            "  Simulating High Intensity scenario...\n",
            "\n",
            "📊 MONTE CARLO SIMULATION RESULTS\n",
            "---------------------------------------------\n",
            "National ROI Statistics by Scenario:\n",
            "                  Mean ROI  Median ROI  Std Dev  5th Percentile  \\\n",
            "scenario                                                          \n",
            "High Intensity      -0.374      -0.414    0.282          -0.770   \n",
            "Low Intensity       -0.358      -0.408    0.342          -0.828   \n",
            "Medium Intensity    -0.394      -0.425    0.277          -0.802   \n",
            "\n",
            "                  95th Percentile  \n",
            "scenario                           \n",
            "High Intensity              0.159  \n",
            "Low Intensity               0.290  \n",
            "Medium Intensity            0.113  \n",
            "\n",
            "🎯 PROBABILITY OF POSITIVE ROI:\n",
            "  Low Intensity  : 14.3%\n",
            "  Medium Intensity: 8.4%\n",
            "  High Intensity : 10.5%\n",
            "\n",
            "💰 EXPECTED REVENUE RECOVERY (RM millions):\n",
            "  Low Intensity  : RM 4.5M ± 2.4M\n",
            "  Medium Intensity: RM 8.5M ± 3.9M\n",
            "  High Intensity : RM 17.5M ± 7.9M\n",
            "\n",
            "💾 Saved simulation results to: data/processed/monte_carlo_enforcement_simulation.csv\n",
            "💾 Saved summary statistics to: data/processed/enforcement_roi_summary.csv\n"
          ]
        }
      ],
      "source": [
        "# MONTE CARLO SIMULATION FOR ENFORCEMENT ROI\n",
        "print(\"\\n🎲 MONTE CARLO SIMULATION - ENFORCEMENT ROI ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 8. ENFORCEMENT SCENARIO SIMULATION\n",
        "print(\"\\n💼 ENFORCEMENT SCENARIO MODELING\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Define simulation parameters\n",
        "np.random.seed(42)\n",
        "n_simulations = 1000\n",
        "\n",
        "# Enforcement scenarios\n",
        "scenarios = {\n",
        "    'Low Intensity': {'reduction_mean': 0.05, 'reduction_std': 0.02, 'cost_per_state': 500000},\n",
        "    'Medium Intensity': {'reduction_mean': 0.10, 'reduction_std': 0.03, 'cost_per_state': 1000000},\n",
        "    'High Intensity': {'reduction_mean': 0.20, 'reduction_std': 0.05, 'cost_per_state': 2000000}\n",
        "}\n",
        "\n",
        "# Economic parameters (with uncertainty)\n",
        "excise_rate_mean = 0.40  # RM per stick\n",
        "excise_rate_std = 0.05\n",
        "price_elasticity_mean = 0.3  # How many illegal consumers switch to legal\n",
        "price_elasticity_std = 0.1\n",
        "\n",
        "# Load economic analysis data\n",
        "economic_df = pd.read_csv('data/processed/economic_impact_by_state.csv')\n",
        "\n",
        "def run_monte_carlo_simulation(scenario_name, scenario_params):\n",
        "    \"\"\"Run Monte Carlo simulation for a specific enforcement scenario\"\"\"\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for sim in range(n_simulations):\n",
        "        # Sample uncertain parameters\n",
        "        excise_rate = max(0.1, np.random.normal(excise_rate_mean, excise_rate_std))\n",
        "        elasticity = max(0.1, min(0.8, np.random.normal(price_elasticity_mean, price_elasticity_std)))\n",
        "        reduction_rate = max(0.01, min(0.5, np.random.normal(\n",
        "            scenario_params['reduction_mean'], \n",
        "            scenario_params['reduction_std']\n",
        "        )))\n",
        "        \n",
        "        # Calculate state-level impacts\n",
        "        state_results = []\n",
        "        total_enforcement_cost = 0\n",
        "        total_recovered_revenue = 0\n",
        "        \n",
        "        for _, state_row in economic_df.iterrows():\n",
        "            # Current illegal volume\n",
        "            illegal_sticks = state_row['illegal_sticks_000'] * 1000  # Convert to actual sticks\n",
        "            \n",
        "            # Enforcement impact\n",
        "            reduced_illegal = illegal_sticks * reduction_rate\n",
        "            \n",
        "            # Revenue recovery (accounting for elasticity)\n",
        "            # Some illegal consumers switch to legal (generating tax revenue)\n",
        "            # Others quit smoking (no revenue)\n",
        "            legal_switchers = reduced_illegal * elasticity\n",
        "            revenue_recovered = legal_switchers * excise_rate * 1.1  # Include sales tax\n",
        "            \n",
        "            # Enforcement cost\n",
        "            enforcement_cost = scenario_params['cost_per_state']\n",
        "            \n",
        "            # State ROI\n",
        "            state_roi = (revenue_recovered - enforcement_cost) / enforcement_cost if enforcement_cost > 0 else 0\n",
        "            \n",
        "            state_results.append({\n",
        "                'state': state_row['state'],\n",
        "                'revenue_recovered': revenue_recovered,\n",
        "                'enforcement_cost': enforcement_cost,\n",
        "                'roi': state_roi,\n",
        "                'illegal_reduction': reduced_illegal\n",
        "            })\n",
        "            \n",
        "            total_enforcement_cost += enforcement_cost\n",
        "            total_recovered_revenue += revenue_recovered\n",
        "        \n",
        "        # National ROI\n",
        "        national_roi = (total_recovered_revenue - total_enforcement_cost) / total_enforcement_cost\n",
        "        \n",
        "        results.append({\n",
        "            'simulation': sim,\n",
        "            'scenario': scenario_name,\n",
        "            'national_roi': national_roi,\n",
        "            'total_revenue_recovered': total_recovered_revenue,\n",
        "            'total_enforcement_cost': total_enforcement_cost,\n",
        "            'reduction_rate': reduction_rate,\n",
        "            'elasticity': elasticity,\n",
        "            'excise_rate': excise_rate\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Run simulations for all scenarios\n",
        "print(\"Running Monte Carlo simulations...\")\n",
        "all_results = []\n",
        "\n",
        "for scenario_name, params in scenarios.items():\n",
        "    print(f\"  Simulating {scenario_name} scenario...\")\n",
        "    scenario_results = run_monte_carlo_simulation(scenario_name, params)\n",
        "    all_results.append(scenario_results)\n",
        "\n",
        "# Combine results\n",
        "simulation_df = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "# Analysis of results\n",
        "print(\"\\n📊 MONTE CARLO SIMULATION RESULTS\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "summary_stats = simulation_df.groupby('scenario')['national_roi'].agg([\n",
        "    'mean', 'median', 'std', \n",
        "    lambda x: np.percentile(x, 5),   # 5th percentile\n",
        "    lambda x: np.percentile(x, 95)   # 95th percentile\n",
        "]).round(3)\n",
        "\n",
        "summary_stats.columns = ['Mean ROI', 'Median ROI', 'Std Dev', '5th Percentile', '95th Percentile']\n",
        "\n",
        "print(\"National ROI Statistics by Scenario:\")\n",
        "print(summary_stats)\n",
        "\n",
        "# Probability of positive ROI\n",
        "print(f\"\\n🎯 PROBABILITY OF POSITIVE ROI:\")\n",
        "for scenario in scenarios.keys():\n",
        "    scenario_data = simulation_df[simulation_df['scenario'] == scenario]\n",
        "    prob_positive = (scenario_data['national_roi'] > 0).mean()\n",
        "    print(f\"  {scenario:15s}: {prob_positive:.1%}\")\n",
        "\n",
        "# Revenue recovery analysis\n",
        "print(f\"\\n💰 EXPECTED REVENUE RECOVERY (RM millions):\")\n",
        "revenue_stats = simulation_df.groupby('scenario')['total_revenue_recovered'].agg(['mean', 'std']).round(0)\n",
        "for scenario in scenarios.keys():\n",
        "    mean_rev = revenue_stats.loc[scenario, 'mean'] / 1_000_000\n",
        "    std_rev = revenue_stats.loc[scenario, 'std'] / 1_000_000\n",
        "    print(f\"  {scenario:15s}: RM {mean_rev:.1f}M ± {std_rev:.1f}M\")\n",
        "\n",
        "# Save simulation results\n",
        "simulation_df.to_csv('data/processed/monte_carlo_enforcement_simulation.csv', index=False)\n",
        "summary_stats.to_csv('data/processed/enforcement_roi_summary.csv')\n",
        "\n",
        "print(f\"\\n💾 Saved simulation results to: data/processed/monte_carlo_enforcement_simulation.csv\")\n",
        "print(f\"💾 Saved summary statistics to: data/processed/enforcement_roi_summary.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellUniqueIdByVincent": "c7f78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 INTERACTIVE DASHBOARD CREATION\n",
            "============================================================\n",
            "\n",
            "🗺️ INTERACTIVE SPATIAL MAPS\n",
            "------------------------------\n",
            "✅ Saved interactive choropleth map: outputs/figures/interactive_incidence_map.html\n",
            "\n",
            "📈 TIME SERIES DASHBOARD\n",
            "-------------------------\n",
            "✅ Saved interactive time series: outputs/figures/interactive_timeseries.html\n",
            "\n",
            "💰 ENFORCEMENT ROI DASHBOARD\n",
            "------------------------------\n",
            "✅ Saved ROI simulation dashboard: outputs/figures/interactive_roi_dashboard.html\n",
            "\n",
            "🏷️ BRAND INTELLIGENCE DASHBOARD\n",
            "-----------------------------------\n",
            "✅ Saved brand intelligence treemap: outputs/figures/interactive_brand_treemap.html\n",
            "\n",
            "============================================================\n",
            "📊 INTERACTIVE DASHBOARD SUMMARY\n",
            "============================================================\n",
            "Created interactive visualizations:\n",
            "1. ✅ Spatial Incidence Choropleth Map\n",
            "2. ✅ Multi-State Time Series Dashboard\n",
            "3. ✅ Monte Carlo ROI Analysis Dashboard\n",
            "4. ✅ Brand Intelligence Treemap\n",
            "\n",
            "🌐 All dashboards saved as HTML files in outputs/figures/\n",
            "📱 Open in web browser for full interactivity!\n",
            "\n",
            "📄 Saved comprehensive report: outputs/reports/analysis_summary_report.md\n"
          ]
        }
      ],
      "source": [
        "# INTERACTIVE DASHBOARD CREATION WITH PLOTLY/FOLIUM\n",
        "print(\"\\n📊 INTERACTIVE DASHBOARD CREATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 9. INTERACTIVE SPATIAL VISUALIZATION\n",
        "print(\"\\n🗺️ INTERACTIVE SPATIAL MAPS\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Create interactive choropleth map with Plotly\n",
        "fig_choropleth = px.choropleth(\n",
        "    market_volume_df[market_volume_df['state'] != 'National'],\n",
        "    locations='state',\n",
        "    color='incidence_percent',\n",
        "    hover_name='state',\n",
        "    hover_data={\n",
        "        'illegal_sticks_000': ':,',\n",
        "        'total_sticks_000': ':,',\n",
        "        'risk_level': True\n",
        "    },\n",
        "    color_continuous_scale='Reds',\n",
        "    title='Illegal Cigarette Incidence by Malaysian State (Jan 2024)',\n",
        "    labels={'incidence_percent': 'Illegal Incidence (%)'}\n",
        ")\n",
        "\n",
        "fig_choropleth.update_layout(\n",
        "    geo=dict(\n",
        "        showframe=False,\n",
        "        showcoastlines=True,\n",
        "        projection_type='equirectangular'\n",
        "    ),\n",
        "    title_x=0.5,\n",
        "    width=800,\n",
        "    height=600\n",
        ")\n",
        "\n",
        "# Save interactive map\n",
        "fig_choropleth.write_html('outputs/figures/interactive_incidence_map.html')\n",
        "print(\"✅ Saved interactive choropleth map: outputs/figures/interactive_incidence_map.html\")\n",
        "\n",
        "# 10. TIME SERIES DASHBOARD\n",
        "print(\"\\n📈 TIME SERIES DASHBOARD\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Create multi-state time series plot\n",
        "fig_timeseries = go.Figure()\n",
        "\n",
        "# Add traces for high-risk states\n",
        "high_risk_states_list = ['Pahang', 'Sabah', 'Sarawak', 'Terengganu']\n",
        "colors = ['red', 'orange', 'darkred', 'crimson']\n",
        "\n",
        "for i, state in enumerate(high_risk_states_list):\n",
        "    state_data = trend_df[trend_df['state'] == state].copy()\n",
        "    \n",
        "    # Convert periods to numeric for plotting\n",
        "    period_order = ['2019', '2020', '2021', '2022', '2023', 'Sep_2023', 'Nov_2023', 'Jan_2024']\n",
        "    state_data['period_num'] = state_data['period'].map({p: i for i, p in enumerate(period_order)})\n",
        "    state_data = state_data.sort_values('period_num')\n",
        "    \n",
        "    fig_timeseries.add_trace(go.Scatter(\n",
        "        x=state_data['period'],\n",
        "        y=state_data['illegal_incidence_percent'],\n",
        "        mode='lines+markers',\n",
        "        name=state,\n",
        "        line=dict(color=colors[i], width=3),\n",
        "        marker=dict(size=8)\n",
        "    ))\n",
        "\n",
        "# Add national trend\n",
        "national_data = trend_df[trend_df['state'] == 'National'].copy()\n",
        "national_data['period_num'] = national_data['period'].map({p: i for i, p in enumerate(period_order)})\n",
        "national_data = national_data.sort_values('period_num')\n",
        "\n",
        "fig_timeseries.add_trace(go.Scatter(\n",
        "    x=national_data['period'],\n",
        "    y=national_data['illegal_incidence_percent'],\n",
        "    mode='lines+markers',\n",
        "    name='National Average',\n",
        "    line=dict(color='black', width=4, dash='dash'),\n",
        "    marker=dict(size=10)\n",
        "))\n",
        "\n",
        "fig_timeseries.update_layout(\n",
        "    title='Illegal Cigarette Incidence Trends - High Risk States vs National',\n",
        "    xaxis_title='Time Period',\n",
        "    yaxis_title='Illegal Incidence (%)',\n",
        "    hovermode='x unified',\n",
        "    width=900,\n",
        "    height=500,\n",
        "    legend=dict(x=0.02, y=0.98)\n",
        ")\n",
        "\n",
        "# Save time series plot\n",
        "fig_timeseries.write_html('outputs/figures/interactive_timeseries.html')\n",
        "print(\"✅ Saved interactive time series: outputs/figures/interactive_timeseries.html\")\n",
        "\n",
        "# 11. ENFORCEMENT ROI SIMULATION DASHBOARD\n",
        "print(\"\\n💰 ENFORCEMENT ROI DASHBOARD\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Create ROI distribution plots\n",
        "fig_roi = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=('ROI Distribution by Scenario', 'Revenue Recovery by Scenario',\n",
        "                   'Probability of Positive ROI', 'Cost-Benefit Analysis'),\n",
        "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        ")\n",
        "\n",
        "# ROI Distribution (Box plots)\n",
        "for i, scenario in enumerate(scenarios.keys()):\n",
        "    scenario_data = simulation_df[simulation_df['scenario'] == scenario]\n",
        "    fig_roi.add_trace(\n",
        "        go.Box(y=scenario_data['national_roi'], name=scenario, boxpoints='outliers'),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "# Revenue Recovery (Violin plots)\n",
        "for i, scenario in enumerate(scenarios.keys()):\n",
        "    scenario_data = simulation_df[simulation_df['scenario'] == scenario]\n",
        "    fig_roi.add_trace(\n",
        "        go.Violin(y=scenario_data['total_revenue_recovered']/1_000_000, name=scenario),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "# Probability of positive ROI (Bar chart)\n",
        "prob_data = []\n",
        "for scenario in scenarios.keys():\n",
        "    scenario_data = simulation_df[simulation_df['scenario'] == scenario]\n",
        "    prob_positive = (scenario_data['national_roi'] > 0).mean()\n",
        "    prob_data.append({'scenario': scenario, 'probability': prob_positive})\n",
        "\n",
        "prob_df = pd.DataFrame(prob_data)\n",
        "fig_roi.add_trace(\n",
        "    go.Bar(x=prob_df['scenario'], y=prob_df['probability'], name='Positive ROI Probability'),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Cost-Benefit scatter\n",
        "for scenario in scenarios.keys():\n",
        "    scenario_data = simulation_df[simulation_df['scenario'] == scenario].sample(100)  # Sample for clarity\n",
        "    fig_roi.add_trace(\n",
        "        go.Scatter(\n",
        "            x=scenario_data['total_enforcement_cost']/1_000_000,\n",
        "            y=scenario_data['total_revenue_recovered']/1_000_000,\n",
        "            mode='markers',\n",
        "            name=f'{scenario} Simulations',\n",
        "            opacity=0.6\n",
        "        ),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "fig_roi.update_layout(\n",
        "    title_text=\"Monte Carlo Enforcement ROI Analysis Dashboard\",\n",
        "    showlegend=True,\n",
        "    height=800,\n",
        "    width=1200\n",
        ")\n",
        "\n",
        "# Update axis labels\n",
        "fig_roi.update_xaxes(title_text=\"Scenario\", row=1, col=1)\n",
        "fig_roi.update_yaxes(title_text=\"ROI\", row=1, col=1)\n",
        "fig_roi.update_xaxes(title_text=\"Scenario\", row=1, col=2)\n",
        "fig_roi.update_yaxes(title_text=\"Revenue (RM Millions)\", row=1, col=2)\n",
        "fig_roi.update_xaxes(title_text=\"Scenario\", row=2, col=1)\n",
        "fig_roi.update_yaxes(title_text=\"Probability\", row=2, col=1)\n",
        "fig_roi.update_xaxes(title_text=\"Enforcement Cost (RM Millions)\", row=2, col=2)\n",
        "fig_roi.update_yaxes(title_text=\"Revenue Recovered (RM Millions)\", row=2, col=2)\n",
        "\n",
        "# Save ROI dashboard\n",
        "fig_roi.write_html('outputs/figures/interactive_roi_dashboard.html')\n",
        "print(\"✅ Saved ROI simulation dashboard: outputs/figures/interactive_roi_dashboard.html\")\n",
        "\n",
        "# 12. BRAND INTELLIGENCE DASHBOARD\n",
        "print(\"\\n🏷️ BRAND INTELLIGENCE DASHBOARD\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Create brand market share visualization\n",
        "fig_brands = px.treemap(\n",
        "    top_brands_df,\n",
        "    path=['state', 'brand'],\n",
        "    values='market_share_percent',\n",
        "    title='Illegal Brand Market Share by State (Treemap)',\n",
        "    color='market_share_percent',\n",
        "    color_continuous_scale='Reds'\n",
        ")\n",
        "\n",
        "fig_brands.update_layout(width=1000, height=600)\n",
        "fig_brands.write_html('outputs/figures/interactive_brand_treemap.html')\n",
        "print(\"✅ Saved brand intelligence treemap: outputs/figures/interactive_brand_treemap.html\")\n",
        "\n",
        "# Summary of created dashboards\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📊 INTERACTIVE DASHBOARD SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(\"Created interactive visualizations:\")\n",
        "print(\"1. ✅ Spatial Incidence Choropleth Map\")\n",
        "print(\"2. ✅ Multi-State Time Series Dashboard\")\n",
        "print(\"3. ✅ Monte Carlo ROI Analysis Dashboard\")\n",
        "print(\"4. ✅ Brand Intelligence Treemap\")\n",
        "print(\"\\n🌐 All dashboards saved as HTML files in outputs/figures/\")\n",
        "print(\"📱 Open in web browser for full interactivity!\")\n",
        "\n",
        "# Create summary report\n",
        "summary_report = f\"\"\"\n",
        "# ILLICIT CIGARETTES STUDY - ANALYSIS SUMMARY REPORT\n",
        "\n",
        "## 🎯 KEY FINDINGS\n",
        "\n",
        "### Spatial Hotspots\n",
        "- **4 HIGH RISK states**: Pahang (80.7%), Sarawak (80.3%), Sabah (78.9%), Terengganu (70.5%)\n",
        "- **Geographic clustering**: East Malaysia and East Coast Peninsula show highest incidence\n",
        "- **Moran's I**: {morans_i:.4f} (positive spatial autocorrelation detected)\n",
        "\n",
        "### Temporal Trends\n",
        "- **National incidence**: 56.4% (Jan 2024)\n",
        "- **Significant decreases**: Perlis (-8.8pp), Kedah (-14.0pp)\n",
        "- **Prophet forecasts**: High-risk states trending upward (↗️)\n",
        "\n",
        "### Economic Impact\n",
        "- **Daily tax loss**: RM 796,137\n",
        "- **Annual tax loss**: RM 290.6 million\n",
        "- **Top impact states**: Johor (RM 58.2M), Selangor (RM 45.1M), Pahang (RM 40.7M)\n",
        "\n",
        "### Enforcement ROI\n",
        "- **Low Intensity**: 14.3% chance of positive ROI, RM 4.5M ± 2.4M recovery\n",
        "- **Medium Intensity**: 8.4% chance of positive ROI, RM 8.5M ± 3.9M recovery\n",
        "- **High Intensity**: 10.5% chance of positive ROI, RM 17.5M ± 7.9M recovery\n",
        "\n",
        "## 📊 DELIVERABLES\n",
        "- 7 structured datasets\n",
        "- 4 interactive dashboards\n",
        "- Advanced forecasting models\n",
        "- Monte Carlo simulation results\n",
        "- Spatial clustering analysis\n",
        "\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\"\"\"\n",
        "\n",
        "with open('outputs/reports/analysis_summary_report.md', 'w') as f:\n",
        "    f.write(summary_report)\n",
        "\n",
        "print(f\"\\n📄 Saved comprehensive report: outputs/reports/analysis_summary_report.md\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "vincent": {
      "sessionId": "f6f508d2ec67703736507d1e_2025-11-29T08-07-11-969Z"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
